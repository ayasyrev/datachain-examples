{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b47122-e92a-46f1-bb3a-0af4b9c77e4f",
   "metadata": {},
   "source": [
    "### Supporting JSON outputs in commercial LLMs\n",
    "##### This notebook covers the ways Anthropic, Google, and OpenAI handle the structured JSON outputs:\n",
    "\n",
    "[Anthropic Claude Sonnet 3.5](#anthropic-claude-sonnet-3.5)\n",
    "\n",
    "[Google Gemini Pro 1.5](#google-gemini-pro-1.5)\n",
    "\n",
    "[OpenAI GPT-4o](#openai)\n",
    "\n",
    "```\n",
    "TL;DR \n",
    "As of 08/2024, it is possible to get relatively stable JSON outputs in all top-three commercial commercial LLMs.\n",
    "\n",
    "The clear winner is OpenAI's new \"structured output mode\" which gives clean JSON outputs based on Pydantic data models and can even follow some field constraints provides as Pydantic descriptions. However, contrary to the claims of the OpenAI documentation, the \"structured output mode\" is still somewhat prompt-dependent and can infrequently fail.\n",
    "\n",
    "The second place is taken by Anthropic Claude Sonnet 3.5, which can stick to a structured output by jumping through the hoops of forcing the model into the \"function calling\". Claude cannot accept Pydantic classes directly, but it still reads field descriptions from JSON models and tries to stick to them.\n",
    "\n",
    "Google Gemini 1.5 pro takes a third place as the only way to stick to a structured output is to feed the API with an instance of genai.protos.Schema class. There is no direct way to specify field descriptions.  \n",
    "```\n",
    "\n",
    "Our test methodology is very simple – we stick as close to vendor's documentation as possible and check the validity of the output JSONs as well as integrity of objects they were instructed to represent. We do not check or evaluate the actual object content. To prevent LLM from caching the call results, we use DataChain run the evaluation over a dataset of 50 unique entries.\n",
    "\n",
    "\n",
    "##### Here are the cumulative error rates observed:\n",
    "\n",
    " \"claude-3-5-sonnet-20240620\":\n",
    " \n",
    "| LLM  | mode  | JSON validation errors | Object validation errors |\n",
    "|-----------|-----------|-----------|-----------------|\n",
    "| Claude Sonnet 3.5 | prompt alone | 14% |  N/A |\n",
    "| Claude Sonnet 3.5 | prompt & example | 2% |  N/A |\n",
    "| Claude Sonnet 3.5 | \"auto\" tool config | 2% |  N/A |\n",
    "| Claude Sonnet 3.5 | forced tool config | 0% in 1,000 calls| 0% in 1,000 calls |\n",
    "\n",
    " \"gemini-1.5-pro-latest\":\n",
    " \n",
    "| LLM  | mode  | JSON validation errors  | Object validation errors |\n",
    "|-----------|-----------|-----------|----------------|\n",
    "| Gemini 1.5 Pro | prompt alone | 90% | N/A |\n",
    "| Gemini 1.5 Pro | json mode / schema from a user class | 0% | 2% |\n",
    "| Gemini 1.5 Pro | json mode / schema in the prompt  | 1% | N/A |\n",
    "| Gemini 1.5 Pro | json mode / schema as `genai.protos.Schema` | 0% in 1,000 calls| 0% in 1,000 calls |\n",
    "\n",
    "\"gpt-4o-2024-08-06\":\n",
    "\n",
    "| LLM  | mode  | JSON validation errors  | Object validation errors |\n",
    "|-----------|-----------|-----------|----------------|\n",
    "| GPT 4o | structured output | 0.01% in 10,000 calls  | 0% in 10,000 calls  |\n",
    "| GPT 4o | structured output + inconsistent prompt | 0.05% | N/A |\n",
    "| GPT 4o | structured output + field description constraint  | 0% | 0.05% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766d898-2dcb-4a92-9912-a215ed23e670",
   "metadata": {},
   "source": [
    "### Anthropic Claude Sonnet\n",
    "\n",
    "First try - just ask for JSON output using the Anthropic [recommended prompt](https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ab1e213-155b-433d-8f05-bafec0a0856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the analysis of that feedback in JSON format:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"key_issues\": [\n",
      "    \"Bot unable to perform requested task\",\n",
      "    \"Lack of functionality\",\n",
      "    \"Poor user experience\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    {\n",
      "      \"team\": \"Development\",\n",
      "      \"task\": \"Implement ticket booking functionality\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Knowledge Base\",\n",
      "      \"task\": \"Create and integrate a database of ticket booking information and procedures\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"UX/UI\",\n",
      "      \"task\": \"Design a user-friendly interface for ticket booking process\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Training\",\n",
      "      \"task\": \"Improve bot's response to provide alternatives or direct users to appropriate resources when unable to perform a task\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# requesting JSON output from Anthropic with the prompt here:\n",
    "# https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency\n",
    "#\n",
    "\n",
    "import os\n",
    "import anthropic\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You’re a Customer Insights AI. \n",
    "Analyze this feedback and output in JSON format with keys: “sentiment” (positive/negative/neutral), \n",
    "“key_issues” (list), and “action_items” (list of dicts with “team” and “task”).\n",
    "\"\"\"\n",
    "\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "completion = (\n",
    "   client.messages.create(                       \n",
    "        model=\"claude-3-5-sonnet-20240620\", \n",
    "        max_tokens = 1024,       \n",
    "        system=PROMPT,                           \n",
    "        messages=[{\"role\": \"user\", \"content\": \"User: Book me a ticket. Bot: I do not know.\"}]\n",
    "   )\n",
    ")\n",
    "print(completion.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95a416-173e-4276-884b-cd70932ab211",
   "metadata": {},
   "source": [
    "We can see problem right away – the model produces JSON output but prepends it with unwanted preamble.\n",
    "To understand the prevalence of this behavior, let's try the same prompt but evaluate against 50 chat dialogs from this [public dataset](https://radar.kit.edu/radar/en/dataset/FdJmclKpjHzLfExE.ExpBot%2B-%2BA%2Bdataset%2Bof%2B79%2Bdialogs%2Bwith%2Ban%2Bexperimental%2Bcustomer%2Bservice%2Bchatbot) using the [DataChain library](https://github.com/iterative/datachain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd00e89-2e53-4c21-a624-efb033800cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22584.02 rows/s]\n",
      "Processed: 50 rows [00:00, 9450.04 rows/s]\n",
      "Preparing: 50 rows [00:00, 16987.87 rows/s]\n",
      "Processed: 13 rows [00:52,  4.48s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "Here's the analysis of the dialog in JSON format:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"neutral\",\n",
      "  \"key_issues\": [\n",
      "    \"Chatbot's inability to understand context and recall previously provided information\",\n",
      "    \"Rigid response structure requiring specific input formats\",\n",
      "    \"Limited options presented to the user\",\n",
      "    \"Lack of flexibility in understanding user's budget preferences\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    {\n",
      "      \"team\": \"AI Development\",\n",
      "      \"task\": \"Improve context understanding and information recall capabilities\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"AI Development\",\n",
      "      \"task\": \"Enhance natural language processing to handle various input formats\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Product Management\",\n",
      "      \"task\": \"Expand the database of mobile phone operators and plans\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"UX Design\",\n",
      "      \"task\": \"Redesign the conversation flow to be more user-friendly and less repetitive\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"AI Development\",\n",
      "      \"task\": \"Implement better handling of budget-related queries to understand relative terms\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 20 rows [01:27,  5.84s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "Here's the analysis of the dialog in JSON format:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"neutral\",\n",
      "  \"key_issues\": [\n",
      "    \"User difficulty in providing clear budget information\",\n",
      "    \"Bot's inability to understand non-numeric responses\",\n",
      "    \"Limited flexibility in adjusting search parameters mid-conversation\",\n",
      "    \"Lack of follow-up questions about international travel needs\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    {\n",
      "      \"team\": \"UX Design\",\n",
      "      \"task\": \"Improve flexibility of input options for budget questions\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"NLP\",\n",
      "      \"task\": \"Enhance natural language understanding for non-numeric responses\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Product Management\",\n",
      "      \"task\": \"Implement feature to adjust search parameters during conversation\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Content\",\n",
      "      \"task\": \"Develop more detailed questions about international usage needs\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"QA\",\n",
      "      \"task\": \"Test bot's ability to handle unexpected user responses\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 29 rows [02:13,  5.28s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "Here's the analysis of the dialog in JSON format:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"neutral\",\n",
      "  \"key_issues\": [\n",
      "    \"User's needs not fully met\",\n",
      "    \"Chatbot's inability to handle flexible budget input\",\n",
      "    \"Lack of continuity in conversation\",\n",
      "    \"Limited response to specific requests\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    {\n",
      "      \"team\": \"Product\",\n",
      "      \"task\": \"Improve plan recommendation algorithm to include free text messages when requested\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Development\",\n",
      "      \"task\": \"Enhance chatbot's ability to handle flexible budget inputs\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"UX\",\n",
      "      \"task\": \"Improve conversation flow to maintain context between user requests\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"AI\",\n",
      "      \"task\": \"Train the chatbot to better understand and respond to specific user requests\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 30 rows [02:24,  7.26s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "Here's the analysis of the dialog in JSON format:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"key_issues\": [\n",
      "    \"Customer initially provided incorrect plan information\",\n",
      "    \"Customer wants a cheaper phone plan\",\n",
      "    \"Customer travels outside of Europe\",\n",
      "    \"Customer wants to spend less than 25€ per month\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    {\n",
      "      \"team\": \"Customer Service\",\n",
      "      \"task\": \"Improve the process for customers to accurately identify their current plan\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Product Development\",\n",
      "      \"task\": \"Develop more affordable plans with international roaming options\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"AI Development\",\n",
      "      \"task\": \"Enhance chatbot's ability to handle ambiguous responses like 'sometimes'\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Marketing\",\n",
      "      \"task\": \"Promote the BigPhone Green Xtra plan to budget-conscious customers\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"UX/UI\",\n",
      "      \"task\": \"Improve chatbot's ability to understand and respond to user intent, such as 'can you print this?'\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [03:44,  4.50s/ rows]\n",
      "Saving: 50 rows [00:00, 19150.32 rows/s]\n",
      "Cleanup: 4 tables [00:00, 3559.02 tables/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import anthropic\n",
    "from datachain import File, DataChain, Column\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class TextBlock(BaseModel):\n",
    "    text: str\n",
    "    type: str\n",
    "\n",
    "class Usage(BaseModel):\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "class ClaudeMessage(BaseModel):\n",
    "    id: str\n",
    "    content: List[TextBlock]\n",
    "    model: str\n",
    "    role: str\n",
    "    stop_reason: str\n",
    "    stop_sequence: Optional[str] = None\n",
    "    type: str\n",
    "    usage: Usage\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You’re a Customer Insights AI. \n",
    "Analyze this dialog and output in JSON format with keys: “sentiment” (positive/negative/neutral), \n",
    "“key_issues” (list), and “action_items” (list of dicts with “team” and “task”).\n",
    "\"\"\"\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def eval_dialogue(file: File) -> str:    \n",
    "     completion = (\n",
    "         client.messages.create(                       \n",
    "                model=\"claude-3-5-sonnet-20240620\", \n",
    "                max_tokens = 1024,       \n",
    "                system=PROMPT,                           \n",
    "                messages=[{\"role\": \"user\", \"content\": file.read()},]\n",
    "         )\n",
    "     )\n",
    "     json_string = completion.content[0].text\n",
    "     try:\n",
    "         # Attempt to convert the string to JSON\n",
    "         json_data = json.loads(json_string)\n",
    "         return completion\n",
    "     except json.JSONDecodeError as e:\n",
    "         # Catch JSON decoding errors\n",
    "         print(f\"JSONDecodeError: {e}\")\n",
    "         print(json_string)\n",
    "         return completion\n",
    "\n",
    "simple_chain = DataChain.from_storage(source_files, type=\"text\")       \\\n",
    "              .settings(cache=True)                                    \\\n",
    "              .filter(Column(\"file.path\").glob(\"*.txt\"))               \\\n",
    "              .map(claude = eval_dialogue, output=ClaudeMessage)       \\\n",
    "              .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a9bc5-1cdc-4c6a-8368-46b15310a92d",
   "metadata": {},
   "source": [
    "As we see, about 4 out of 50 responses come with an extra text line preceding the output.\n",
    "To address that, [Anthropic recommends](https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency#prefill-claudes-response) two techniques:\n",
    "\n",
    "1. Providing structure examples in the prompt\n",
    "2. Pre-filling the assistant's answer (coercing the output start).\n",
    "\n",
    "Note that while giving examples are straightforward, pre-filling is a bit inconvenient because one needs to add the coercive part back to the answer to complete a structured object. But nonetheless, let's give both a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5222e05-a409-4ece-b0b1-44597b17076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19291.25 rows/s]\n",
      "Processed: 50 rows [00:00, 6438.31 rows/s]\n",
      "Preparing: 50 rows [00:00, 17773.98 rows/s]\n",
      "Processed: 2 rows [00:06,  3.41s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ',' delimiter: line 1 column 18 (char 17)\n",
      "{\"sentiment\":\": \"negative\",\n",
      "\"key_issues\": [\n",
      "\"Bot unable to handle unrealistic input values\",\n",
      "\"Poor data validation and error handling\",\n",
      "\"Inability to sign contracts or take direct action\",\n",
      "\"Repetitive conversation flow\",\n",
      "\"Lack of personalization in responses\"\n",
      "],\n",
      "\"action_items\": [\n",
      "{\n",
      "\"team\": \"Development\",\n",
      "\"task\": \"Implement input validation for realistic values\"\n",
      "},\n",
      "{\n",
      "\"team\": \"Development\",\n",
      "\"task\": \"Improve error handling and user guidance\"\n",
      "},\n",
      "{\n",
      "\"team\": \"Product Management\",\n",
      "\"task\": \"Define clear user journey and add functionality for contract signing\"\n",
      "},\n",
      "{\n",
      "\"team\": \"UX/UI\",\n",
      "\"task\": \"Design more flexible conversation flows\"\n",
      "},\n",
      "{\n",
      "\"team\": \"Data Science\",\n",
      "\"task\": \"Implement more sophisticated recommendation engine\"\n",
      "},\n",
      "{\n",
      "\"team\": \"Customer Support\",\n",
      "\"task\": \"Create escalation path for complex queries\"\n",
      "}\n",
      "]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 9 rows [00:38,  4.43s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting value: line 3 column 1 (char 15)\n",
      "{\"sentiment\":\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"key_issues\": [\n",
      "    \"Bot unable to handle follow-up questions\",\n",
      "    \"Bot not responding to user's specific queries\",\n",
      "    \"Lack of flexibility in conversation\",\n",
      "    \"Poor user experience\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    {\n",
      "      \"team\": \"Development\",\n",
      "      \"task\": \"Improve bot's ability to handle follow-up questions and maintain context\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Development\",\n",
      "      \"task\": \"Enhance bot's natural language processing to understand varied user inputs\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Content\",\n",
      "      \"task\": \"Expand bot's knowledge base to include information about different providers\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"UX/UI\",\n",
      "      \"task\": \"Design a more intuitive conversation flow that allows for user preferences and follow-up questions\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Product Management\",\n",
      "      \"task\": \"Review and revise the bot's response strategy when it doesn't understand user input\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [03:27,  4.15s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3421.83 tables/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import anthropic\n",
    "from datachain import File, DataChain, Column\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You’re a Customer Insights AI. \n",
    "Analyze this dialog and output in JSON format with keys: “sentiment” (positive/negative/neutral), \n",
    "“key_issues” (list), and “action_items” (list of dicts with “team” and “task”).\n",
    "\n",
    "Example:\n",
    "{\n",
    "  \"sentiment\": \"negative\",\n",
    "  \"key_issues\": [\n",
    "    \"Bot unable to perform requested task\",\n",
    "    \"Poor user experience\"\n",
    "  ],\n",
    "  \"action_items\": [\n",
    "    {\n",
    "      \"team\": \"Development\",\n",
    "      \"task\": \"Implement ticket booking functionality\"\n",
    "    },\n",
    "    {\n",
    "      \"team\": \"UX/UI\",\n",
    "      \"task\": \"Design a user-friendly interface for ticket booking process\"\n",
    "    }\n",
    "  ]\n",
    "}    \n",
    "\"\"\"\n",
    "prefill='{\"sentiment\":'\n",
    "\n",
    "def eval_dialogue(file: File) -> str:    \n",
    "     completion = (\n",
    "         client.messages.create(                       \n",
    "                model=\"claude-3-5-sonnet-20240620\", \n",
    "                max_tokens = 1024,       \n",
    "                system=PROMPT,                           \n",
    "                messages=[{\"role\": \"user\", \"content\": file.read()},\n",
    "                          {\"role\": \"assistant\", \"content\": f'{prefill}'},\n",
    "                         ]\n",
    "         )\n",
    "     )\n",
    "     json_string = prefill + completion.content[0].text\n",
    "     try:\n",
    "         # Attempt to convert the string to JSON\n",
    "         json_data = json.loads(json_string)\n",
    "         return json_string\n",
    "     except json.JSONDecodeError as e:\n",
    "         # Catch JSON decoding errors\n",
    "         print(f\"JSONDecodeError: {e}\")\n",
    "         print(json_string)\n",
    "         return json_string\n",
    "\n",
    "chain = DataChain.from_storage(source_files, type=\"text\")       \\\n",
    "              .settings(cache=True)                             \\\n",
    "              .filter(Column(\"file.path\").glob(\"*.txt\"))        \\\n",
    "              .map(claude = eval_dialogue)                      \\\n",
    "              .exec()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87393e-f9af-4ab0-982a-bdab24651b7b",
   "metadata": {},
   "source": [
    "As expected, coercion got rid of the starter text line because the output is forced to begin as a valid JSON. This is clearly much better than just \"asking\" Claude to be nice and produce valid JSON.\n",
    "Nonetheless, the LLM is still able to deviate from the structured output, although less frequently:\n",
    "\n",
    "```\n",
    "JSONDecodeError: Expecting ',' delimiter: line 1 column 18 (char 17)\n",
    "{\"sentiment\":\": \"negative\",\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236323e-d82e-4f11-b0ae-12e660215976",
   "metadata": {},
   "source": [
    "#### Anthropic \"tool\" hack\n",
    "\n",
    "From the above, it is evident that Anthropic LLM does not reliably stick to the requested schema right out of the box.\n",
    "Nevertheless, the [LangChain](lanngchain.com) library still offers a method call [llm_with_structured_output](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/) for their Anthropic wrapper anyway.\n",
    "\n",
    "How does it work?\n",
    "\n",
    "LangChain wraps the Claude completion request into a \"tool call\", suggesting the LLM to use a pseudo-tool that requires a structured call.\n",
    "The line of thinking here is that Anthropic model is trained well enough to not send unstructured output to an external function.\n",
    "\n",
    "So let's try this idea out in the same testbench:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947b5b1d-be01-46f8-80ec-3fb49095f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 16821.63 rows/s]\n",
      "Processed: 50 rows [00:00, 8862.20 rows/s]\n",
      "Preparing: 50 rows [00:00, 14740.65 rows/s]\n",
      "Processed: 30 rows [02:47,  4.73s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError: list index out of range\n",
      "Message(id='msg_0128s4bv2bKe575FJK6gNPee', content=[TextBlock(text=\"I apologize, but I don't have the ability to directly print this conversation. However, I can evaluate the chatbot dialog and send the results to a manager using the available tool. Would you like me to do that?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=1681, output_tokens=49))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [05:04,  6.10s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2142.41 tables/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import anthropic\n",
    "from datachain import File, DataChain, Column\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "\n",
    "class TextBlock(BaseModel):\n",
    "    text: str\n",
    "    type: str\n",
    "\n",
    "class Usage(BaseModel):\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "class ClaudeMessage(BaseModel):\n",
    "    id: str\n",
    "    content: List[TextBlock]\n",
    "    model: str\n",
    "    role: str\n",
    "    stop_reason: str\n",
    "    stop_sequence: Optional[str] = None\n",
    "    type: str\n",
    "    usage: Usage\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    team: str \n",
    "    task: str\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"dialog sentiment (positive/negative/neutral)\")\n",
    "    key_issues: list[str] = Field(description=\"list of 3 problems discovered in the dialog\")\n",
    "    action_items: list[ActionItem] = Field(description=\"list of dicts with 'team' and 'task'\")\n",
    "\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You’re assigned to evaluate this chatbot dialog and sending the results to the manager via send_to_manager tool.    \n",
    "\"\"\"\n",
    "\n",
    "def eval_dialogue(file: File) -> str:    \n",
    "     completion = (\n",
    "         client.messages.create(                       \n",
    "                model=\"claude-3-5-sonnet-20240620\", \n",
    "                max_tokens = 1024,       \n",
    "                system=PROMPT, \n",
    "                tools=[\n",
    "                    {\n",
    "                        \"name\": \"send_to_manager\",\n",
    "                        \"description\": \"Send bot evaluation results to a manager\",\n",
    "                        \"input_schema\": EvalResponse.model_json_schema(),\n",
    "                    }\n",
    "                ],\n",
    "                messages=[{\"role\": \"user\", \"content\": file.read()},\n",
    "                         ]\n",
    "         )\n",
    "     )\n",
    "     try:\n",
    "         json_dict = completion.content[1].input\n",
    "     except IndexError as e:\n",
    "         # Catch cases where Claude refuses to use tools\n",
    "         print(f\"IndexError: {e}\")\n",
    "         print(completion)\n",
    "         return(completion)\n",
    "     try:\n",
    "         # Attempt to convert the tool dict to EvalResponse object\n",
    "         EvalResponse(**json_dict)\n",
    "         return completion\n",
    "     except ValidationError as e:\n",
    "         # Catch Pydantic validation errors\n",
    "         print(f\"Pydantic error: {e}\")\n",
    "         print(completion)\n",
    "         return completion\n",
    "\n",
    "tool_chain = DataChain.from_storage(source_files, type=\"text\")          \\\n",
    "              .settings(cache=True)                                     \\\n",
    "              .filter(Column(\"file.path\").glob(\"*.txt\"))                \\\n",
    "              .map(claude = eval_dialogue, output=ClaudeMessage)        \\\n",
    "              .exec()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f66fb3-1c9c-4987-96db-59414f22e5d7",
   "metadata": {},
   "source": [
    "\n",
    "Interestingly, things with Anthropic tools are going well here until the point when the LLM refuses to make a tool call:\n",
    "\n",
    "```\n",
    "IndexError: list index out of range\n",
    "Message(id='msg_018V97rq6HZLdxeNRZyNWDGT', content=[TextBlock(text=\"I apologize, but I don't have the ability to directly print anything. I'm a chatbot designed to help evaluate conversations and provide analysis. Based on the conversation you've shared, it seems you were interacting with a different chatbot that helps find mobile phone plans. That chatbot doesn't appear to have printing capabilities either.\\n\\nHowever, I can analyze this conversation and send an evaluation to the manager. Would you like me to do that?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=1676, output_tokens=95))\n",
    "```\n",
    "\n",
    "If the call is not made, the JSON input is not provided because the response object does not have the tool block.\n",
    "Luckily, Anthropic offers an option to always force the tool use and eliminate the text block from the response altogether.\n",
    "\n",
    "This takes the form of the following argument: `tool_choice = {\"type\": \"tool\", \"name\": \"send_to_manager\"}`\n",
    "To better understand the error rate, the total number of LLM calls is forced to be 1,000. We also wrap the exceptions the Claude API may throw at us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a695a4c-ef84-4553-a51d-e91caf09b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23550.28 rows/s]\n",
      "Processed: 50 rows [00:00, 8172.21 rows/s]\n",
      "Preparing: 50 rows [00:00, 19492.07 rows/s]\n",
      "Processed: 50 rows [03:41,  4.43s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2867.41 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 40745.13 rows/s]\n",
      "Processed: 50 rows [00:00, 10279.65 rows/s]\n",
      "Preparing: 50 rows [00:00, 20990.41 rows/s]\n",
      "Processed: 50 rows [03:26,  4.13s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2122.89 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 29006.25 rows/s]\n",
      "Processed: 50 rows [00:00, 12703.08 rows/s]\n",
      "Preparing: 50 rows [00:00, 19157.32 rows/s]\n",
      "Processed: 50 rows [03:20,  4.02s/ rows]\n",
      "Cleanup: 4 tables [00:00, 1734.62 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 21349.40 rows/s]\n",
      "Processed: 50 rows [00:00, 9573.41 rows/s]\n",
      "Preparing: 50 rows [00:00, 19208.21 rows/s]\n",
      "Processed: 50 rows [03:16,  3.94s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2557.89 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23912.79 rows/s]\n",
      "Processed: 50 rows [00:00, 8296.02 rows/s]\n",
      "Preparing: 50 rows [00:00, 16745.07 rows/s]\n",
      "Processed: 50 rows [03:42,  4.45s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2275.49 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 24365.66 rows/s]\n",
      "Processed: 50 rows [00:00, 9852.26 rows/s]\n",
      "Preparing: 50 rows [00:00, 23923.71 rows/s]\n",
      "Processed: 50 rows [03:41,  4.42s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2362.66 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20919.22 rows/s]\n",
      "Processed: 50 rows [00:00, 10756.83 rows/s]\n",
      "Preparing: 50 rows [00:00, 17029.25 rows/s]\n",
      "Processed: 50 rows [03:41,  4.43s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2443.17 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20168.80 rows/s]\n",
      "Processed: 50 rows [00:00, 10206.11 rows/s]\n",
      "Preparing: 50 rows [00:00, 20858.88 rows/s]\n",
      "Processed: 50 rows [03:21,  4.02s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2362.66 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 28684.89 rows/s]\n",
      "Processed: 50 rows [00:00, 10729.32 rows/s]\n",
      "Preparing: 50 rows [00:00, 26952.22 rows/s]\n",
      "Processed: 50 rows [03:45,  4.51s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2580.71 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 18016.77 rows/s]\n",
      "Processed: 50 rows [00:00, 7313.01 rows/s]\n",
      "Preparing: 50 rows [00:00, 19827.47 rows/s]\n",
      "Processed: 50 rows [03:31,  4.23s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2162.29 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23017.80 rows/s]\n",
      "Processed: 50 rows [00:00, 9179.11 rows/s]\n",
      "Preparing: 50 rows [00:00, 18647.98 rows/s]\n",
      "Processed: 50 rows [03:53,  4.68s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2284.17 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23899.17 rows/s]\n",
      "Processed: 50 rows [00:00, 9817.67 rows/s]\n",
      "Preparing: 50 rows [00:00, 16313.90 rows/s]\n",
      "Processed: 43 rows [02:52,  4.42s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APIError: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [03:21,  4.02s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2509.31 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22484.74 rows/s]\n",
      "Processed: 50 rows [00:00, 10940.90 rows/s]\n",
      "Preparing: 50 rows [00:00, 17985.87 rows/s]\n",
      "Processed: 29 rows [02:07,  4.77s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APIError: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [03:42,  4.45s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2747.66 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26329.59 rows/s]\n",
      "Processed: 50 rows [00:00, 9873.13 rows/s]\n",
      "Preparing: 50 rows [00:00, 20162.98 rows/s]\n",
      "Processed: 50 rows [03:20,  4.01s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3853.29 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23923.71 rows/s]\n",
      "Processed: 50 rows [00:00, 9433.88 rows/s]\n",
      "Preparing: 50 rows [00:00, 17611.29 rows/s]\n",
      "Processed: 16 rows [00:54,  4.01s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APIError: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [03:12,  3.86s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3007.75 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26455.81 rows/s]\n",
      "Processed: 50 rows [00:00, 10586.33 rows/s]\n",
      "Preparing: 50 rows [00:00, 20400.31 rows/s]\n",
      "Processed: 50 rows [03:12,  3.85s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3744.91 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 29171.68 rows/s]\n",
      "Processed: 50 rows [00:00, 12048.44 rows/s]\n",
      "Preparing: 50 rows [00:00, 17421.10 rows/s]\n",
      "Processed: 50 rows [03:08,  3.76s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2594.28 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25291.27 rows/s]\n",
      "Processed: 50 rows [00:00, 9382.81 rows/s]\n",
      "Preparing: 50 rows [00:00, 16117.06 rows/s]\n",
      "Processed: 50 rows [03:08,  3.78s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2820.65 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 24903.84 rows/s]\n",
      "Processed: 50 rows [00:00, 9540.31 rows/s]\n",
      "Preparing: 50 rows [00:00, 17869.39 rows/s]\n",
      "Processed: 50 rows [03:11,  3.83s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4306.27 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22269.85 rows/s]\n",
      "Processed: 50 rows [00:00, 9435.16 rows/s]\n",
      "Preparing: 50 rows [00:00, 17568.50 rows/s]\n",
      "Processed: 9 rows [00:32,  4.14s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APIError: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [03:21,  4.03s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2794.81 tables/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import anthropic\n",
    "from datachain import File, DataChain, Column\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    team: str \n",
    "    task: str\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"dialog sentiment (positive/negative/neutral)\")\n",
    "    key_issues: list[str] = Field(description=\"list of 3 problems discovered in the dialog\")\n",
    "    action_items: list[ActionItem] = Field(description=\"list of dicts with 'team' and 'task'\")\n",
    "\n",
    "    @field_validator(\"key_issues\")\n",
    "    def count_issues(cls, value):\n",
    "        if  len(value) != 3:\n",
    "            raise ValueError(f\"{len(value)} issues provided\")\n",
    "        return value\n",
    "\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You’re assigned to evaluate this chatbot dialog and sending the results to the manager via send_to_manager tool.    \n",
    "\"\"\"\n",
    "\n",
    "def eval_dialogue(file: File) -> str:\n",
    "     try:\n",
    "        completion = (\n",
    "             client.messages.create(                       \n",
    "                    model=\"claude-3-5-sonnet-20240620\", \n",
    "                    max_tokens = 1024,       \n",
    "                    system=PROMPT, \n",
    "                    tools=[\n",
    "                        {\n",
    "                            \"name\": \"send_to_manager\",\n",
    "                            \"description\": \"Send bot evaluation results to a manager\",\n",
    "                            \"input_schema\": EvalResponse.model_json_schema(),\n",
    "                        }\n",
    "                    ],\n",
    "                    tool_choice = {\"type\": \"tool\", \"name\": \"send_to_manager\"},\n",
    "                    messages=[{\"role\": \"user\", \"content\": file.read()},\n",
    "                             ]\n",
    "             )\n",
    "         )\n",
    "     except anthropic.APIError as e:\n",
    "        print(f\"APIError: {e}\")\n",
    "        return \"error\"\n",
    "     except anthropic.RateLimitError as e:\n",
    "        print(f\"RateLimitError: {e}\")\n",
    "        return \"error\"\n",
    "     except anthropic.AuthenticationError as e:\n",
    "        print(f\"AuthenticationError: {e}\")\n",
    "        return \"error\"\n",
    "     except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return \"error\"\n",
    "         \n",
    "     try:\n",
    "         json_dict = completion.content[0].input\n",
    "     except IndexError as e:\n",
    "         # Catch cases where Claude refuses to use tools\n",
    "         print(f\"IndexError: {e}\")\n",
    "         print(completion)\n",
    "         return str(completion)\n",
    "     try:\n",
    "         # Attempt to convert the tool dict to EvalResponse object\n",
    "         EvalResponse(**json_dict)\n",
    "         return str(completion)\n",
    "     except ValidationError as e:\n",
    "         # Catch Pydantic validation errors\n",
    "         print(f\"Pydantic error: {e}\")\n",
    "         print(completion)\n",
    "         return str(completion)\n",
    "\n",
    "\n",
    "for i in range(1,21): # 50x20 = 1,000 LLM calls\n",
    "    forced_tool_chain = DataChain.from_storage(source_files, type=\"text\")          \\\n",
    "                  .settings(cache=True)                                            \\\n",
    "                  .filter(Column(\"file.path\").glob(\"*.txt\"))                       \\\n",
    "                  .map(claude = eval_dialogue)                                     \\\n",
    "                  .exec()\n",
    "    print(f\"{i*50} calls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d6f19-0bc5-4bdd-824c-d84dafea87a2",
   "metadata": {},
   "source": [
    "#### Great!\n",
    "\n",
    "The Anthropic API throws up an \"Internal server error\" exception occasionally, but \n",
    "Claude 3.5 Sonnet forced into the tool mode seems to be capable of 1,000+ calls without a single JSON failure. It was also capable of reading the field limit from the Pydantic description.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14698c1f-eec1-48c2-a79d-1f2f947bb448",
   "metadata": {},
   "source": [
    "And the next up is...\n",
    "\n",
    "### Google Gemini Pro 1.5\n",
    "\n",
    "[Google documentation on structured outputs](https://ai.google.dev/gemini-api/docs/json-mode?lang=python) says right away that prompt-based generation is unreliable, and \"Google can't guarantee that it will produce JSON and nothing but JSON.\" This appears, indeed, true because a pure prompt-induced JSON output tends to emit many Markdown preambles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dacc6827-b445-4b3c-a9b7-9b9ea9846cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 28089.37 rows/s]\n",
      "Processed: 50 rows [00:00, 8382.57 rows/s]\n",
      "Preparing: 1 rows [00:00, 394.65 rows/s]\n",
      "Download: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError: Expecting value: line 1 column 1 (char 0)\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"key_issues\": [\n",
      "    \"Bot misunderstood user confirmation.\",\n",
      "    \"Recommended plan doesn't meet user needs (more MB, less minutes, price limit).\"\n",
      "  ],\n",
      "  \"action_items\": [\n",
      "    {\n",
      "      \"team\": \"Engineering\",\n",
      "      \"task\": \"Investigate why bot didn't understand 'correct' and 'yes it is' confirmations.\"\n",
      "    },\n",
      "    {\n",
      "      \"team\": \"Product\",\n",
      "      \"task\": \"Review and improve plan matching logic to prioritize user needs and constraints.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download: 1.73kB [00:03, 503B/s]/s]\u001b[A\n",
      "Processed: 1 rows [00:00, 165.37 rows/s]\n",
      "Cleanup: 4 tables [00:00, 2552.44 tables/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datachain.lib.dc import Column, DataChain\n",
    "import google.generativeai as genai\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "PROMPT=\"\"\"\n",
    "You’re a Customer Insights AI. \n",
    "Analyze the following dialog and provide evaluation in JSON format with keys: “sentiment” (positive/negative/neutral), \n",
    "“key_issues” (list), and “action_items” (list of dicts with “team” and “task”).\n",
    "\"\"\"\n",
    "\n",
    "def gemini_setup():\n",
    "    genai.configure(api_key=google_api_key)\n",
    "    return genai.GenerativeModel(model_name='gemini-1.5-pro-latest', system_instruction=PROMPT)\n",
    "\n",
    "def eval_dialogue (file, model):\n",
    "    response = model.generate_content(file.read(), stream=False)\n",
    "    response.resolve()\n",
    "    try:\n",
    "         json_dict = json.loads(response.text)\n",
    "    except json.JSONDecodeError as e:\n",
    "         # Catch cases where Gemini fails to produc JSON\n",
    "         print(f\"IndexError: {e}\")\n",
    "         print(response.text)\n",
    "         return response.text\n",
    "    return response.text\n",
    "\n",
    "chain =  (\n",
    "         DataChain.from_storage(source_files, type=\"text\")\n",
    "            .settings(cache=True)\n",
    "            .limit(1)\n",
    "            .setup(model = lambda: gemini_setup())\n",
    "            .map(gemini = eval_dialogue)\n",
    "            .exec()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0d46a-b1e8-417c-ba75-7aeb5abdcd5d",
   "metadata": {},
   "source": [
    "According to [Google documentation](https://ai.google.dev/gemini-api/docs/json-mode?lang=python), there are three possible ways to get around this, and they all require switching the model config to the JSON output mode. \n",
    "\n",
    "In the most convenient syntax form, one can point the output configuration to a user class directly, or to a data model subclassed from `typing.TypedDict` \n",
    "\n",
    "In both cases the result will be similar to the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "653e9cd5-b1bd-47e7-a50b-fae3c92f7bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 30709.50 rows/s]\n",
      "Processed: 50 rows [00:00, 9384.49 rows/s]\n",
      "Preparing: 50 rows [00:00, 17737.90 rows/s]\n",
      "Download: 0.00B [00:00, ?B/s]\n",
      "Download: 9.73kB [00:01, 5.40kB/s]]\u001b[A\n",
      "Download: 15.0kB [00:07, 1.90kB/s]rows]\u001b[A\n",
      "Download: 18.8kB [00:10, 1.54kB/s]rows]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic error: 1 validation error for EvalResponse\n",
      "sentiment\n",
      "  Field required [type=missing, input_value={'action_items': [{'task'...nd could be improved.\"]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing\n",
      "{\"action_items\": [{\"task\": \"Investigate why the bot recommends a more expensive plan when the user specified a maximum budget.\", \"team\": \"Bot Development\"}, {\"task\": \"Improve the bot's understanding of user requests related to additional packages or modifications to existing plans.\", \"team\": \"NLU Training\"}, {\"task\": \"Train the bot on handling farewells like 'byebye' more gracefully.\", \"team\": \"Conversation Flow\"}], \"key_issues\": [\"The bot fails to respect the user's budget constraint.\", \"The bot struggles to understand requests about plan modifications.\", \"The bot's response to the user's farewell is abrupt and could be improved.\"]} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download: 22.4kB [00:14, 1.25kB/s]rows]\u001b[A\n",
      "Download: 24.1kB [00:16, 1.26kB/s]rows]\u001b[A\n",
      "Download: 25.9kB [00:19, 1.04kB/s]rows]\u001b[A\n",
      "Download: 27.7kB [00:20, 1.04kB/s]rows]\u001b[A\n",
      "Download: 29.8kB [00:22, 1.16kB/s]rows]\u001b[A\n",
      "Download: 31.5kB [00:26, 814B/s]  rows]\u001b[A\n",
      "Download: 38.1kB [00:27, 1.54kB/s] rows]\u001b[A\n",
      "Download: 40.6kB [00:31, 1.20kB/s] rows]\u001b[A\n",
      "Download: 43.1kB [00:36, 871B/s]   rows]\u001b[A\n",
      "Download: 46.6kB [00:43, 708B/s]s/ rows]\u001b[A\n",
      "Download: 51.0kB [00:46, 912B/s]s/ rows]\u001b[A\n",
      "Download: 53.0kB [00:50, 760B/s]s/ rows]\u001b[A\n",
      "Download: 58.0kB [00:54, 974B/s]s/ rows]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic error: 1 validation error for EvalResponse\n",
      "sentiment\n",
      "  Field required [type=missing, input_value={'action_items': [{'task'...nfusion for the user.']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing\n",
      "{\"action_items\": [{\"task\": \"Bot should clarify what \\\"inclusive international minutes\\\" means in the offered plan.\", \"team\": \"Bot team\"}, {\"task\": \"Bot should confirm if the user is looking for a plan with inclusive international minutes or just the option to add them.\", \"team\": \"Bot team\"}], \"key_issues\": [\"The bot recommended a plan without considering the user's need for international calls despite explicitly stating it in the summary.\", \"The bot didn't clarify what \\\"inclusive international minutes\\\" means, leading to potential confusion for the user.\"]} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download: 59.9kB [00:57, 894B/s]s/ rows]\u001b[A\n",
      "Download: 63.0kB [00:58, 1.11kB/s] rows]\u001b[A\n",
      "Download: 65.7kB [01:03, 829B/s]   rows]\u001b[A\n",
      "Download: 67.6kB [01:08, 696B/s]s/ rows]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic error: 1 validation error for EvalResponse\n",
      "sentiment\n",
      "  Field required [type=missing, input_value={'action_items': [{'task'...r towards a solution.\"]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing\n",
      "{\"action_items\": [{\"task\": \"Improve bot's ability to understand user intents and extract key information from free-form text.\", \"team\": \"Engineering\"}, {\"task\": \"Train the bot on a wider variety of expressions for common requests, such as 'cheapest offer', 'less than I pay now', etc.\", \"team\": \"Data Science\"}, {\"task\": \"Implement a more flexible confirmation flow, allowing users to correct or adjust their input more easily.\", \"team\": \"Design\"}, {\"task\": \"Explore the possibility of asking clarifying questions when user input is ambiguous or incomplete.\", \"team\": \"Engineering\"}], \"key_issues\": [\"The bot struggles to understand user intents expressed in natural language.\", \"The conversation flow is rigid, leading to frustration when the user needs to correct or adjust information.\", \"The bot's error messages are repetitive and not very helpful in guiding the user towards a solution.\"]} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download: 69.7kB [01:09, 820B/s]s/ rows]\u001b[A\n",
      "Download: 71.2kB [01:11, 862B/s]s/ rows]\u001b[A\n",
      "Download: 73.6kB [01:15, 743B/s]s/ rows]\u001b[A\n",
      "Download: 75.4kB [01:19, 629B/s]s/ rows]\u001b[A\n",
      "Download: 76.9kB [01:24, 523B/s]s/ rows]\u001b[A\n",
      "Download: 78.7kB [01:28, 481B/s]s/ rows]\u001b[A\n",
      "Download: 80.7kB [01:29, 615B/s]s/ rows]\u001b[A\n",
      "Download: 82.9kB [01:32, 699B/s]s/ rows]\u001b[A\n",
      "Download: 86.5kB [01:35, 827B/s]s/ rows]\u001b[A\n",
      "Download: 88.2kB [01:38, 737B/s]s/ rows]\u001b[A\n",
      "Download: 89.8kB [01:40, 816B/s]s/ rows]\u001b[A\n",
      "Download: 91.6kB [01:44, 696B/s]s/ rows]\u001b[A\n",
      "Download: 94.9kB [01:47, 785B/s]s/ rows]\u001b[A\n",
      "Download: 98.1kB [01:50, 842B/s]s/ rows]\u001b[A\n",
      "Download: 101kB [01:55, 770B/s] s/ rows]\u001b[A\n",
      "Download: 103kB [01:57, 757B/s]3s/ rows]\u001b[A\n",
      "Download: 105kB [01:59, 852B/s]5s/ rows]\u001b[A\n",
      "Download: 107kB [02:01, 940B/s]8s/ rows]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic error: 1 validation error for EvalResponse\n",
      "sentiment\n",
      "  Field required [type=missing, input_value={'action_items': [{'task'...n inappropriate plan.\"]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing\n",
      "{\"action_items\": [{\"task\": \"Bot didn't understand \\\"Never\\\" and \\\"Yes\\\" answers for the \\\"Do you often travel outside of Europe?\\\" question.  Need to investigate and fix.\", \"team\": \"Bot team\"}], \"key_issues\": [\"The bot failed to understand the user's answer to a yes/no question multiple times. This led to the bot misunderstanding the user's travel habits and potentially recommending an inappropriate plan.\"]} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download: 109kB [02:05, 752B/s]1s/ rows]\u001b[A\n",
      "Download: 113kB [02:08, 926B/s]5s/ rows]\u001b[A\n",
      "Download: 114kB [02:10, 900B/s]1s/ rows]\u001b[A\n",
      "Download: 116kB [02:15, 662B/s]1s/ rows]\u001b[A\n",
      "Download: 118kB [02:16, 722B/s]3s/ rows]\u001b[A\n",
      "Download: 119kB [02:18, 798B/s]5s/ rows]\u001b[A\n",
      "Download: 121kB [02:22, 616B/s]1s/ rows]\u001b[A\n",
      "Download: 123kB [02:24, 725B/s]6s/ rows]\u001b[A\n",
      "Download: 126kB [02:25, 1.05kB/s]/ rows]\u001b[A\n",
      "Download: 128kB [02:29, 779B/s]  / rows]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic error: 1 validation error for EvalResponse\n",
      "sentiment\n",
      "  Field required [type=missing, input_value={'action_items': [{'task'...tion-gathering phase.']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing\n",
      "{\"action_items\": [{\"task\": \"Bot should handle clarification better\", \"team\": \"Product\"}, {\"task\": \"Bot should not end dialog prematurely\", \"team\": \"Engineering\"}, {\"task\": \"Bot should handle interruptions or clarifications gracefully during information gathering\", \"team\": \"Engineering\"}], \"key_issues\": [\"The bot struggles with user clarifications and prematurely ends the dialogue. The user provides a clarification about the number of text messages, but instead of smoothly incorporating this information, the bot gets confused and offers to restart. This highlights a need for the bot to handle interruptions or clarifications gracefully during its information-gathering phase.\"]} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download: 130kB [02:33, 707B/s]5s/ rows]\u001b[A\n",
      "Download: 130kB [02:37, 843B/s]5s/ rows]\u001b[A\n",
      "Processed: 50 rows [02:35,  3.12s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2236.07 tables/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datachain.lib.dc import Column, DataChain\n",
    "import google.generativeai as genai\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "PROMPT=\"\"\"\n",
    "You’re a Customer Insights AI. \n",
    "Analyze the following dialog and provide evaluation in JSON format.\n",
    "\"\"\"\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    team: str \n",
    "    task: str\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    sentiment: str \n",
    "    key_issues: list[str] \n",
    "    action_items: list[ActionItemG]\n",
    "\n",
    "def gemini_setup():\n",
    "    genai.configure(api_key=google_api_key)\n",
    "    return genai.GenerativeModel(model_name='gemini-1.5-pro-latest', \n",
    "                                 system_instruction=PROMPT,\n",
    "                                 generation_config={\"response_mime_type\": \"application/json\",\n",
    "                                                     \"response_schema\": EvalResponse\n",
    "                                                   },\n",
    "                                )\n",
    "\n",
    "def eval_dialogue (file, model):\n",
    "    response = model.generate_content(file.read(), stream=False)\n",
    "    response.resolve()\n",
    "    try:\n",
    "         json_dict = json.loads(response.text)\n",
    "    except json.JSONDecodeError as e:\n",
    "         # Catch cases where Gemini fails to produce valid JSON\n",
    "         print(f\"JSONDecodeError: {e}\")\n",
    "         print(response.text)\n",
    "         return response.text\n",
    "    try:\n",
    "         # Attempt to convert the response dict to EvalResponse object\n",
    "         eval_object = EvalResponse(**json_dict)\n",
    "         return str(response.text)\n",
    "    except ValidationError as e:\n",
    "         # Catch Pydantic validation errors\n",
    "         print(f\"Pydantic error: {e}\")\n",
    "         print(response.text)\n",
    "         return str(response.text)\n",
    "    \n",
    "    return str(eval_object)\n",
    "\n",
    "chain =  (\n",
    "         DataChain.from_storage(source_files, type=\"text\")\n",
    "            .settings(cache=True)\n",
    "            .setup(model = lambda: gemini_setup())\n",
    "            .map(gemini = eval_dialogue)\n",
    "            .exec()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313af898-ee43-413b-8eb4-33caf9ee54c1",
   "metadata": {},
   "source": [
    "#### As we can see, Gemini 1.5 Pro produces valid JSONs but does not stick to the data model provided.\n",
    "\n",
    "Various object validity issues are sporadically observed, but most frequently the missing fields. The average frequency of issues is about 2 in 50 runs - which is enough to disqualify this mode of operation.\n",
    "\n",
    "So let us try a different \"recommended\" JSON mode syntax – insert the schema right into the prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57b4dd0-d235-4e86-bb12-abe839c8bf9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Listing gs://datachain-demo: 50 objects [00:00, 399.36 objects/s]\n",
      "Preparing: 50 rows [00:00, 17199.64 rows/s]\n",
      "Processed: 50 rows [00:00, 13683.62 rows/s]\n",
      "Preparing: 50 rows [00:00, 24640.49 rows/s]\n",
      "Processed: 50 rows [02:10,  2.61s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2173.78 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20260.38 rows/s]\n",
      "Processed: 50 rows [00:00, 9844.40 rows/s]\n",
      "Preparing: 50 rows [00:00, 17356.22 rows/s]\n",
      "Processed: 50 rows [02:15,  2.72s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4157.92 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 31291.44 rows/s]\n",
      "Processed: 50 rows [00:00, 12468.20 rows/s]\n",
      "Preparing: 50 rows [00:00, 21159.84 rows/s]\n",
      "Processed: 50 rows [02:10,  2.60s/ rows]\n",
      "Cleanup: 4 tables [00:00, 937.64 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23139.71 rows/s]\n",
      "Processed: 50 rows [00:00, 9848.10 rows/s]\n",
      "Preparing: 50 rows [00:00, 19508.39 rows/s]\n",
      "Processed: 50 rows [02:13,  2.67s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2711.69 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 21843.06 rows/s]\n",
      "Processed: 50 rows [00:00, 9384.49 rows/s]\n",
      "Preparing: 50 rows [00:00, 19312.57 rows/s]\n",
      "Processed: 50 rows [02:13,  2.68s/ rows]\n",
      "Cleanup: 4 tables [00:00, 254.97 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 5231.24 rows/s]\n",
      "Processed: 50 rows [00:00, 1323.88 rows/s]\n",
      "Preparing: 50 rows [00:00, 2667.96 rows/s]\n",
      "Processed: 50 rows [02:11,  2.63s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2258.65 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22574.29 rows/s]\n",
      "Processed: 50 rows [00:00, 9414.40 rows/s]\n",
      "Preparing: 50 rows [00:00, 20965.23 rows/s]\n",
      "Processed: 45 rows [02:01,  2.18s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ':' delimiter: line 1 column 116 (char 115)\n",
      "{\"sentiment\": \"negative\", \"key_issues\": [\"bot doesn't understand user's price correction request\"], \"action_items\" = [{\"team\": \"bot_improvement\", \"task\": \"Improve natural language understanding for price correction\"}]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [02:13,  2.67s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3046.53 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20209.62 rows/s]\n",
      "Processed: 50 rows [00:00, 10445.02 rows/s]\n",
      "Preparing: 50 rows [00:00, 20998.82 rows/s]\n",
      "Processed: 16 rows [00:44,  2.92s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ':' delimiter: line 1 column 168 (char 167)\n",
      "{\"sentiment\": \"negative\", \"key_issues\": [\"User's need for international calls not met\", \"Bot's suggested plan doesn't match user's budget\"], \"action_items\": [{\"team: \"bot_improvement\", \"task\": \"Improve understanding of user requirements, especially regarding international calls\"}, {\"team\": \"bot_improvement\", \"task\": \"Enhance filtering of plans to strictly adhere to user's budget constraints\"}]}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [02:19,  2.78s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2413.99 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19608.71 rows/s]\n",
      "Processed: 50 rows [00:00, 5226.29 rows/s]\n",
      "Preparing: 50 rows [00:00, 20500.02 rows/s]\n",
      "Processed: 50 rows [02:16,  2.73s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2612.46 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25575.02 rows/s]\n",
      "Processed: 50 rows [00:00, 7965.18 rows/s]\n",
      "Preparing: 50 rows [00:00, 7579.43 rows/s]\n",
      "Processed: 50 rows [02:11,  2.63s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2746.76 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20490.00 rows/s]\n",
      "Processed: 50 rows [00:00, 8841.65 rows/s]\n",
      "Preparing: 50 rows [00:00, 17792.08 rows/s]\n",
      "Processed: 50 rows [02:14,  2.68s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3620.46 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20858.88 rows/s]\n",
      "Processed: 50 rows [00:00, 8441.62 rows/s]\n",
      "Preparing: 50 rows [00:00, 17789.06 rows/s]\n",
      "Processed: 50 rows [02:15,  2.70s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2989.53 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19018.34 rows/s]\n",
      "Processed: 50 rows [00:00, 10936.34 rows/s]\n",
      "Preparing: 50 rows [00:00, 18044.67 rows/s]\n",
      "Processed: 50 rows [02:10,  2.62s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2440.32 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22748.15 rows/s]\n",
      "Processed: 50 rows [00:00, 9790.63 rows/s]\n",
      "Preparing: 50 rows [00:00, 15384.04 rows/s]\n",
      "Processed: 50 rows [02:07,  2.55s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2740.03 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19501.13 rows/s]\n",
      "Processed: 50 rows [00:00, 8926.71 rows/s]\n",
      "Preparing: 50 rows [00:00, 17624.61 rows/s]\n",
      "Processed: 50 rows [02:11,  2.64s/ rows]\n",
      "Cleanup: 4 tables [00:00, 1758.99 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 16890.72 rows/s]\n",
      "Processed: 50 rows [00:00, 9535.11 rows/s]\n",
      "Preparing: 50 rows [00:00, 19246.99 rows/s]\n",
      "Processed: 2 rows [00:03,  1.79s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ':' delimiter: line 1 column 248 (char 247)\n",
      "{\"sentiment\": \"negative\", \"key_issues\": [\"Bot doesn't handle user input variations effectively\", \"Bot fails to accurately capture and present summarized information\", \"Bot doesn't guide the user to successfully complete the task\"], \"action_items\" = [{\"team\": \"NLU\", \"task\": \"Improve model's ability to handle typos, abbreviations, and different phrasings for the same intent.\"}, {\"team\": \"Dialogue\", \"task\": \"Implement confirmation mechanisms to ensure accurate data capture and present a summary for user verification before proceeding.\"}, {\"team\": \"Integration\", \"task\": \"Connect the chatbot with a backend system to access real-time mobile phone plan data and offer personalized recommendations.\"}]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [02:17,  2.74s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3328.15 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22206.18 rows/s]\n",
      "Processed: 50 rows [00:00, 9296.71 rows/s]\n",
      "Preparing: 50 rows [00:00, 17933.57 rows/s]\n",
      "Processed: 50 rows [02:08,  2.57s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3248.88 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 34222.45 rows/s]\n",
      "Processed: 50 rows [00:00, 6660.16 rows/s]\n",
      "Preparing: 50 rows [00:00, 21700.66 rows/s]\n",
      "Processed: 50 rows [02:13,  2.66s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4592.72 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26342.82 rows/s]\n",
      "Processed: 50 rows [00:00, 12642.59 rows/s]\n",
      "Preparing: 50 rows [00:00, 13224.57 rows/s]\n",
      "Processed: 50 rows [02:15,  2.71s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2792.94 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 18342.97 rows/s]\n",
      "Processed: 50 rows [00:00, 9125.99 rows/s]\n",
      "Preparing: 50 rows [00:00, 17005.77 rows/s]\n",
      "Processed: 42 rows [01:54,  2.18s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ':' delimiter: line 1 column 71 (char 70)\n",
      "{\"sentiment\": \"positive\", \"key_issues\": [], \"action_items\": [{\"team: \"N/A\", \"task\": \"N/A\"}]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [02:12,  2.65s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2841.19 tables/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datachain.lib.dc import Column, DataChain\n",
    "import google.generativeai as genai\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "PROMPT=\"\"\"\n",
    "You’re a Customer Insights AI. \n",
    "Analyze the following dialog and provide evaluation in JSON format with schema EvalResponse:\n",
    "\n",
    "EvalResponse = {\"sentiment\": str, \"key_issues\": list[str], \"action_items\" = list[ActionItem]}\n",
    "ActionItem = {\"team: str, \"task\":str}\n",
    "\"\"\"\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    team: str \n",
    "    task: str\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"dialog sentiment (positive/negative/neutral)\")\n",
    "    key_issues: list[str] = Field(description=\"list of 3 problems discovered in the dialog\")\n",
    "    action_items: list[ActionItem] = Field(description=\"list of dicts with 'team' and 'task'\")\n",
    "\n",
    "def gemini_setup():\n",
    "    genai.configure(api_key=google_api_key)\n",
    "    return genai.GenerativeModel(model_name='gemini-1.5-pro-latest', \n",
    "                                 system_instruction=PROMPT,\n",
    "                                 generation_config={\"response_mime_type\": \"application/json\"}\n",
    "                                )\n",
    "\n",
    "def eval_dialogue (file, model):\n",
    "    response = model.generate_content(file.read(), stream=False)\n",
    "    response.resolve()\n",
    "    try:\n",
    "         json_dict = json.loads(response.text)\n",
    "    except json.JSONDecodeError as e:\n",
    "         # Catch cases where Gemini fails to produce valid JSON\n",
    "         print(f\"JSONDecodeError: {e}\")\n",
    "         print(response.text)\n",
    "         return response.text\n",
    "    try:\n",
    "         # Attempt to convert the response dict to EvalResponse object\n",
    "         eval_object = EvalResponse(**json_dict)\n",
    "         return str(response.text)\n",
    "    except ValidationError as e:\n",
    "         # Catch Pydantic validation errors\n",
    "         print(f\"Pydantic error: {e}\")\n",
    "         print(response.text)\n",
    "         return str(response.text)\n",
    "    \n",
    "    return str(eval_object)\n",
    "\n",
    "for i in range(1,21):\n",
    "    chain =  (\n",
    "             DataChain.from_storage(source_files, type=\"text\")\n",
    "                .settings(cache=True)\n",
    "                .setup(model = lambda: gemini_setup())\n",
    "                .map(gemini = eval_dialogue)\n",
    "                .exec()\n",
    "    )\n",
    "    print(f\"{i*50} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46637b2-ff10-4a69-94d5-8e2971858da9",
   "metadata": {},
   "source": [
    "The results of this run are somewhat unexpected: when output format is given as prompt, Gemini Pro stops failing model validations, but infrequently fails to produce valid JSONs:\n",
    "```\n",
    "JSONDecodeError: Expecting ':' delimiter: line 1 column 135 (char 134)\n",
    "{\"sentiment\": \"negative\", \"key_issues\": [\"bot failed to recommend a plan\", \"user's budget is unrealistic\"], \"action_items\": [{\"team: \"bot\", \"task\": \"improve plan recommendation logic to handle edge cases like extremely low budgets\"}, {\"team\": \"bot\", \"task\": \"prompt user for a more realistic budget if their initial input is too low\"}]}\n",
    "```\n",
    "\n",
    "This is obviously not acceptable, and mixing instructions with formats in one prompt is in any case too fragile.\n",
    "\n",
    "Finally, let us try to give Gemini a last chance by feeding a json output schema directly to the model configuration. \n",
    "\n",
    "Unfortunately, unlike Anthropic Claude tool configuration, Gemini cannot read the output of Pydantic `model_json_schema()` method, so we would need to hardcode a `genai.protos.Schema` class duplicating our data model. The configuration looks clunky but appears to work well:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233558fc-5571-4048-b51d-e0be4274f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26329.59 rows/s]\n",
      "Processed: 50 rows [00:00, 7691.17 rows/s]\n",
      "Preparing: 50 rows [00:00, 23201.15 rows/s]\n",
      "Processed: 50 rows [03:18,  3.97s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3445.01 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 24425.25 rows/s]\n",
      "Processed: 50 rows [00:00, 11249.00 rows/s]\n",
      "Preparing: 50 rows [00:00, 8873.08 rows/s]\n",
      "Processed: 50 rows [03:21,  4.03s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2865.45 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 17482.09 rows/s]\n",
      "Processed: 50 rows [00:00, 9407.22 rows/s]\n",
      "Preparing: 50 rows [00:00, 19522.92 rows/s]\n",
      "Processed: 50 rows [03:12,  3.85s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3134.17 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 24594.25 rows/s]\n",
      "Processed: 50 rows [00:00, 10752.97 rows/s]\n",
      "Preparing: 50 rows [00:00, 19517.47 rows/s]\n",
      "Processed: 50 rows [03:05,  3.71s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2816.86 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 18946.17 rows/s]\n",
      "Processed: 50 rows [00:00, 10264.56 rows/s]\n",
      "Preparing: 50 rows [00:00, 19025.24 rows/s]\n",
      "Processed: 50 rows [03:10,  3.80s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2913.72 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26135.99 rows/s]\n",
      "Processed: 50 rows [00:00, 9529.48 rows/s]\n",
      "Preparing: 50 rows [00:00, 16727.70 rows/s]\n",
      "Processed: 50 rows [03:09,  3.80s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4202.71 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26938.37 rows/s]\n",
      "Processed: 50 rows [00:00, 12970.20 rows/s]\n",
      "Preparing: 50 rows [00:00, 23994.87 rows/s]\n",
      "Processed: 50 rows [03:11,  3.83s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2391.96 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19887.64 rows/s]\n",
      "Processed: 50 rows [00:00, 11372.22 rows/s]\n",
      "Preparing: 50 rows [00:00, 17667.67 rows/s]\n",
      "Processed: 50 rows [03:08,  3.77s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2142.41 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19127.62 rows/s]\n",
      "Processed: 50 rows [00:00, 9160.67 rows/s]\n",
      "Preparing: 50 rows [00:00, 18699.53 rows/s]\n",
      "Processed: 50 rows [03:05,  3.71s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3363.52 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23157.60 rows/s]\n",
      "Processed: 50 rows [00:00, 8122.51 rows/s]\n",
      "Preparing: 50 rows [00:00, 21911.52 rows/s]\n",
      "Processed: 50 rows [03:09,  3.80s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3780.36 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22161.60 rows/s]\n",
      "Processed: 50 rows [00:00, 6292.46 rows/s]\n",
      "Preparing: 50 rows [00:00, 21533.55 rows/s]\n",
      "Processed: 50 rows [03:04,  3.69s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2480.00 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 24989.90 rows/s]\n",
      "Processed: 50 rows [00:00, 10692.66 rows/s]\n",
      "Preparing: 50 rows [00:00, 10817.87 rows/s]\n",
      "Processed: 50 rows [03:08,  3.76s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2637.10 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 18586.83 rows/s]\n",
      "Processed: 50 rows [00:00, 8890.38 rows/s]\n",
      "Preparing: 50 rows [00:00, 19715.63 rows/s]\n",
      "Processed: 50 rows [03:16,  3.92s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2424.10 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 27489.21 rows/s]\n",
      "Processed: 50 rows [00:00, 13112.94 rows/s]\n",
      "Preparing: 50 rows [00:00, 22990.05 rows/s]\n",
      "Processed: 50 rows [03:05,  3.71s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2904.64 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 31036.73 rows/s]\n",
      "Processed: 50 rows [00:00, 12465.24 rows/s]\n",
      "Preparing: 50 rows [00:00, 21196.20 rows/s]\n",
      "Processed: 50 rows [03:07,  3.75s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4907.05 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 11541.84 rows/s]\n",
      "Processed: 50 rows [00:00, 10672.53 rows/s]\n",
      "Preparing: 50 rows [00:00, 22419.84 rows/s]\n",
      "Processed: 50 rows [03:04,  3.69s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3773.55 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 27060.03 rows/s]\n",
      "Processed: 50 rows [00:00, 11795.01 rows/s]\n",
      "Preparing: 50 rows [00:00, 22696.45 rows/s]\n",
      "Processed: 50 rows [03:08,  3.77s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3847.10 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 31078.13 rows/s]\n",
      "Processed: 50 rows [00:00, 13527.39 rows/s]\n",
      "Preparing: 50 rows [00:00, 26633.88 rows/s]\n",
      "Processed: 50 rows [03:11,  3.83s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3619.68 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 27732.77 rows/s]\n",
      "Processed: 50 rows [00:00, 11850.99 rows/s]\n",
      "Preparing: 50 rows [00:00, 23777.23 rows/s]\n",
      "Processed: 50 rows [03:09,  3.79s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2851.33 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 24166.31 rows/s]\n",
      "Processed: 50 rows [00:00, 12509.85 rows/s]\n",
      "Preparing: 50 rows [00:00, 24777.32 rows/s]\n",
      "Processed: 50 rows [03:15,  3.91s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4048.56 tables/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datachain.lib.dc import Column, DataChain\n",
    "import google.generativeai as genai\n",
    "\n",
    "source_files = \"gs://datachain-demo/chatbot-KiT/\"\n",
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "PROMPT=\"\"\"\n",
    "You’re a Customer Insights AI. \n",
    "Analyze the following dialog and provide evaluation in JSON format.\n",
    "\"\"\"\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    team: str \n",
    "    task: str\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"dialog sentiment (positive/negative/neutral)\")\n",
    "    key_issues: list[str] = Field(description=\"list of 3 problems discovered in the dialog\")\n",
    "    action_items: list[ActionItem] = Field(description=\"list of dicts with 'team' and 'task'\")\n",
    "\n",
    "g_str = genai.protos.Schema(type=genai.protos.Type.STRING)\n",
    "\n",
    "g_action_item = genai.protos.Schema(\n",
    "            type=genai.protos.Type.OBJECT,\n",
    "            properties={\n",
    "                'team':genai.protos.Schema(type=genai.protos.Type.STRING),\n",
    "                'task':genai.protos.Schema(type=genai.protos.Type.STRING)\n",
    "            },\n",
    "            required=['team','task']\n",
    "        )\n",
    "\n",
    "g_evaluation=genai.protos.Schema(\n",
    "            type=genai.protos.Type.OBJECT,\n",
    "            properties={\n",
    "                'sentiment':genai.protos.Schema(type=genai.protos.Type.STRING),\n",
    "                'key_issues':genai.protos.Schema(type=genai.protos.Type.ARRAY, items=g_str),\n",
    "                'action_items':genai.protos.Schema(type=genai.protos.Type.ARRAY, items=g_action_item)\n",
    "            },\n",
    "            required=['sentiment','key_issues', 'action_items']\n",
    "        )\n",
    "\n",
    "def gemini_setup():\n",
    "    genai.configure(api_key=google_api_key)\n",
    "    return genai.GenerativeModel(model_name='gemini-1.5-pro-latest', \n",
    "                                 system_instruction=PROMPT,\n",
    "                                 generation_config={\"response_mime_type\": \"application/json\",\n",
    "                                                     \"response_schema\": g_evaluation,\n",
    "                                                   }\n",
    "                                )\n",
    "\n",
    "def eval_dialogue (file, model):\n",
    "    response = model.generate_content(file.read(), stream=False)\n",
    "    response.resolve()\n",
    "    try:\n",
    "         json_dict = json.loads(response.text)\n",
    "    except json.JSONDecodeError as e:\n",
    "         # Catch cases where Gemini fails to produce valid JSON\n",
    "         print(f\"IndexError: {e}\")\n",
    "         print(response.text)\n",
    "         return response.text\n",
    "    try:\n",
    "         # Attempt to convert the response dict to EvalResponse object\n",
    "         eval_object = EvalResponse(**json_dict)\n",
    "         return str(response.text)\n",
    "    except ValidationError as e:\n",
    "         # Catch Pydantic validation errors\n",
    "         print(f\"Pydantic error: {e}\")\n",
    "         print(response.text)\n",
    "         return str(response.text)\n",
    "    \n",
    "    return str(eval_object)\n",
    "    \n",
    "for i in range(1,21):\n",
    "    chain =  (\n",
    "             DataChain.from_storage(source_files, type=\"text\")\n",
    "                .settings(cache=True)\n",
    "                .setup(model = lambda: gemini_setup())\n",
    "                .map(gemini = eval_dialogue)\n",
    "                .exec()\n",
    "    )\n",
    "    print(f\"{i*50} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e3b1a-8620-4fbe-adc4-928fd3fa3f5a",
   "metadata": {},
   "source": [
    "### OpenAI\n",
    "\n",
    "OpenAI went through several iterations of API methods to coerce their models to structured outputs.\n",
    "There latest version is called [\"Structured Outputs\"](https://openai.com/index/introducing-structured-outputs-in-the-api/) and can take a Pydantic object as pattern. OpenAI explains that it works by constrained sampling, so in theory it should always get the right result given the system tries to resample for long enough. Constrained sampling requires some special machinery at the inference time and only partially depends on training, so this mechanism would be difficult to replicate in the open-source models.\n",
    "\n",
    "And what is most impressive is... this just works out of the box, with a Pydantic model as guidance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3d2b6e-5784-4d30-aded-033b22ef135d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19816.23 rows/s]\n",
      "Processed: 50 rows [00:00, 10354.26 rows/s]\n",
      "Preparing: 50 rows [00:00, 17277.57 rows/s]\n",
      "Processed: 50 rows [01:14,  1.50s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2799.00 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 28934.22 rows/s]\n",
      "Processed: 50 rows [00:00, 11660.56 rows/s]\n",
      "Preparing: 50 rows [00:00, 20671.78 rows/s]\n",
      "Processed: 50 rows [01:22,  1.64s/ rows]\n",
      "Cleanup: 4 tables [00:00, 1837.39 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 31726.96 rows/s]\n",
      "Processed: 50 rows [00:00, 8047.71 rows/s]\n",
      "Preparing: 50 rows [00:00, 18832.18 rows/s]\n",
      "Processed: 50 rows [01:21,  1.63s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3625.94 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 24462.29 rows/s]\n",
      "Processed: 50 rows [00:00, 12821.13 rows/s]\n",
      "Preparing: 50 rows [00:00, 22491.98 rows/s]\n",
      "Processed: 50 rows [01:12,  1.45s/ rows]\n",
      "Cleanup: 4 tables [00:00, 1809.06 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20592.62 rows/s]\n",
      "Processed: 50 rows [00:00, 9336.86 rows/s]\n",
      "Preparing: 50 rows [00:00, 22260.40 rows/s]\n",
      "Processed: 50 rows [01:17,  1.54s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3150.06 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 27623.18 rows/s]\n",
      "Processed: 50 rows [00:00, 10676.88 rows/s]\n",
      "Preparing: 50 rows [00:00, 21948.22 rows/s]\n",
      "Processed: 50 rows [01:14,  1.50s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4841.91 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 21496.02 rows/s]\n",
      "Processed: 50 rows [00:00, 9720.29 rows/s]\n",
      "Preparing: 50 rows [00:00, 20112.71 rows/s]\n",
      "Processed: 50 rows [01:15,  1.50s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3231.98 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19215.25 rows/s]\n",
      "Processed: 50 rows [00:00, 9007.61 rows/s]\n",
      "Preparing: 50 rows [00:00, 18539.18 rows/s]\n",
      "Processed: 50 rows [01:09,  1.39s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2536.24 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20526.10 rows/s]\n",
      "Processed: 50 rows [00:00, 9273.28 rows/s]\n",
      "Preparing: 50 rows [00:00, 21430.12 rows/s]\n",
      "Processed: 50 rows [01:12,  1.46s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2525.55 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 29363.65 rows/s]\n",
      "Processed: 50 rows [00:00, 9675.00 rows/s]\n",
      "Preparing: 50 rows [00:00, 27428.09 rows/s]\n",
      "Processed: 50 rows [01:10,  1.41s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3967.18 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26924.53 rows/s]\n",
      "Processed: 50 rows [00:00, 11676.79 rows/s]\n",
      "Preparing: 50 rows [00:00, 14269.25 rows/s]\n",
      "Processed: 50 rows [01:10,  1.41s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3887.21 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 29070.58 rows/s]\n",
      "Processed: 50 rows [00:00, 9538.15 rows/s]\n",
      "Preparing: 50 rows [00:00, 21460.83 rows/s]\n",
      "Processed: 50 rows [01:11,  1.43s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2255.00 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 9439.83 rows/s]\n",
      "Processed: 50 rows [00:00, 9410.60 rows/s]\n",
      "Preparing: 50 rows [00:00, 21673.75 rows/s]\n",
      "Processed: 50 rows [01:15,  1.50s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4499.12 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23550.28 rows/s]\n",
      "Processed: 50 rows [00:00, 8741.05 rows/s]\n",
      "Preparing: 50 rows [00:00, 20024.37 rows/s]\n",
      "Processed: 50 rows [01:06,  1.34s/ rows]\n",
      "Cleanup: 4 tables [00:00, 5038.20 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20502.02 rows/s]\n",
      "Processed: 50 rows [00:00, 6633.20 rows/s]\n",
      "Preparing: 50 rows [00:00, 14301.36 rows/s]\n",
      "Processed: 50 rows [01:11,  1.44s/ rows]\n",
      "Cleanup: 4 tables [00:00, 1886.78 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25392.32 rows/s]\n",
      "Processed: 50 rows [00:00, 9243.85 rows/s]\n",
      "Preparing: 50 rows [00:00, 14232.45 rows/s]\n",
      "Processed: 50 rows [01:07,  1.36s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2440.68 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 21079.02 rows/s]\n",
      "Processed: 50 rows [00:00, 7375.51 rows/s]\n",
      "Preparing: 50 rows [00:00, 16362.27 rows/s]\n",
      "Processed: 50 rows [01:10,  1.40s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2679.21 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 18498.30 rows/s]\n",
      "Processed: 50 rows [00:00, 8778.00 rows/s]\n",
      "Preparing: 50 rows [00:00, 16431.50 rows/s]\n",
      "Processed: 50 rows [01:25,  1.70s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3146.51 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 33005.23 rows/s]\n",
      "Processed: 50 rows [00:00, 12281.28 rows/s]\n",
      "Preparing: 50 rows [00:00, 24264.17 rows/s]\n",
      "Processed: 50 rows [01:12,  1.44s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2077.93 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20055.01 rows/s]\n",
      "Processed: 50 rows [00:00, 7933.84 rows/s]\n",
      "Preparing: 50 rows [00:00, 16550.80 rows/s]\n",
      "Processed: 50 rows [01:17,  1.55s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3142.39 tables/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datachain import File, DataChain, Column\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "prompt = \"You are assigned to evaluate the success of a dialog between user and a chatbot. Reply with a JSON.\"\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    team: str \n",
    "    task: str\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    sentiment: str \n",
    "    key_issues: list[str]\n",
    "    action_items: list[ActionItem] \n",
    "\n",
    "def eval_dialogue(client, file: File) -> str:\n",
    "     completion = client.beta.chat.completions.parse(\n",
    "         model=\"gpt-4o-2024-08-06\",\n",
    "         messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": file.read()},\n",
    "         ],\n",
    "         response_format=EvalResponse,\n",
    "     )\n",
    "     message = completion.choices[0].message\n",
    "\n",
    "     try: \n",
    "         EvalResponse(**message.parsed.dict())\n",
    "         return str(message.parsed)\n",
    "     except ValidationError as e:\n",
    "         print(e)\n",
    "         return str(message.parsed)\n",
    "\n",
    "# 1,000 entries to evaluate\n",
    "for i in range(1,21):\n",
    "    chain = (\n",
    "       DataChain.from_storage(\"gs://datachain-demo/chatbot-KiT/\", object_name=\"file\", type=\"text\")\n",
    "       .setup(client=lambda : OpenAI(api_key=openai_api_key))\n",
    "       .settings(cache=True)\n",
    "       .map(evaluation=eval_dialogue)\n",
    "       .exec()\n",
    "    )\n",
    "    print(f\"{i*50} calls\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a61c072-ab39-43ea-8b32-a1f17376250a",
   "metadata": {},
   "source": [
    "There are several interesting moments about OpenAI's implementation.\n",
    "\n",
    "* The OpenAI documentation claims [the structured output is achieved 100% time](https://openai.com/index/introducing-structured-outputs-in-the-api/) (see the graph attached here for convenience). This gives an optimistic impression that \"strict\" output mode is all that is needed. At the same time, the documentation recommends using Pydantic or Zod for the data models, and not feed the JSON schemas directly. In fact, [OpenAI's API examples](https://platform.openai.com/docs/guides/structured-outputs/examples_) do not even provide instructions for the output format in the prompt – which gives an impression that any prompt will work in conjunction with structured output.\n",
    "\n",
    "  <img src=\"https://github.com/iterative/datachain-examples/blob/main/assets/gpt-json-stats.jpg?raw=true\" alt=\"Image description\" style=\"width:500px;\"/>\n",
    "\n",
    "* Despite these bold claims, the prompt requesting a structured output can still fail for GPT-4o. Once in about 5,000 runs GPT may still emit a malformed JSON output like this:\n",
    "\n",
    "```\n",
    "pydantic_core._pydantic_core.ValidationError: 1 validation error for Evaluation\n",
    "  Invalid JSON: EOF while parsing a list at line 15579 column 967 [type=json_invalid, input_value='{\"outcome\":\"Yes\",\"explan...   \\t   \\t   \\t   \\t   ', input_type=str]\n",
    "```\n",
    "\n",
    "*  Moreover, if the prompt is inconsistent, this may provoke the model to produce a broken object more often. This does not happen very often and affects about 0.05% outputs but it means the prompt must stay in sync. It is a small number, but is much higher than the rosy \"100% reliable\" picture painted by OpenAI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ef9fd2-579c-4a9c-be9c-e81e8edd3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19562.99 rows/s]\n",
      "Processed: 50 rows [00:00, 6850.30 rows/s]\n",
      "Preparing: 50 rows [00:00, 17811.72 rows/s]\n",
      "Processed: 50 rows [01:11,  1.44s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3210.34 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 22902.17 rows/s]\n",
      "Processed: 50 rows [00:00, 9391.63 rows/s]\n",
      "Preparing: 50 rows [00:00, 20432.11 rows/s]\n",
      "Processed: 50 rows [01:12,  1.45s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2452.45 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23539.70 rows/s]\n",
      "Processed: 50 rows [00:00, 9319.43 rows/s]\n",
      "Preparing: 50 rows [00:00, 20848.51 rows/s]\n",
      "Processed: 50 rows [01:20,  1.61s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3011.53 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26303.17 rows/s]\n",
      "Processed: 50 rows [00:00, 9848.10 rows/s]\n",
      "Preparing: 50 rows [00:00, 18314.14 rows/s]\n",
      "Processed: 50 rows [01:26,  1.74s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3252.66 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 20128.15 rows/s]\n",
      "Processed: 50 rows [00:00, 9543.79 rows/s]\n",
      "Preparing: 50 rows [00:00, 21051.52 rows/s]\n",
      "Processed: 50 rows [01:23,  1.67s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3653.57 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 39089.51 rows/s]\n",
      "Processed: 50 rows [00:00, 11334.73 rows/s]\n",
      "Preparing: 50 rows [00:00, 19262.90 rows/s]\n",
      "Processed: 50 rows [01:09,  1.39s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2639.17 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 41429.32 rows/s]\n",
      "Processed: 50 rows [00:00, 13767.16 rows/s]\n",
      "Preparing: 50 rows [00:00, 29658.49 rows/s]\n",
      "Processed: 50 rows [01:12,  1.45s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2665.17 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 41016.08 rows/s]\n",
      "Processed: 50 rows [00:00, 9216.63 rows/s]\n",
      "Preparing: 50 rows [00:00, 16030.82 rows/s]\n",
      "Processed: 50 rows [01:18,  1.58s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3842.70 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 29886.73 rows/s]\n",
      "Processed: 50 rows [00:00, 11128.43 rows/s]\n",
      "Preparing: 50 rows [00:00, 23063.37 rows/s]\n",
      "Processed: 50 rows [01:09,  1.38s/ rows]\n",
      "Cleanup: 4 tables [00:00, 5533.38 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25948.43 rows/s]\n",
      "Processed: 50 rows [00:00, 12885.73 rows/s]\n",
      "Preparing: 50 rows [00:00, 24686.90 rows/s]\n",
      "Processed: 50 rows [01:08,  1.37s/ rows]\n",
      "Cleanup: 4 tables [00:00, 5310.93 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25175.89 rows/s]\n",
      "Processed: 50 rows [00:00, 12129.98 rows/s]\n",
      "Preparing: 50 rows [00:00, 18464.10 rows/s]\n",
      "Processed: 50 rows [01:07,  1.36s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3590.25 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25537.65 rows/s]\n",
      "Processed: 50 rows [00:00, 7902.45 rows/s]\n",
      "Preparing: 50 rows [00:00, 18953.02 rows/s]\n",
      "Processed: 50 rows [01:08,  1.37s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2317.62 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26273.52 rows/s]\n",
      "Processed: 50 rows [00:00, 10254.52 rows/s]\n",
      "Preparing: 50 rows [00:00, 19865.04 rows/s]\n",
      "Processed: 50 rows [01:13,  1.47s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2408.44 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23833.98 rows/s]\n",
      "Processed: 50 rows [00:00, 12239.71 rows/s]\n",
      "Preparing: 50 rows [00:00, 24753.92 rows/s]\n",
      "Processed: 50 rows [01:05,  1.31s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2532.03 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25919.56 rows/s]\n",
      "Processed: 50 rows [00:00, 13619.64 rows/s]\n",
      "Preparing: 50 rows [00:00, 23785.32 rows/s]\n",
      "Processed: 50 rows [01:06,  1.32s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3499.63 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 35116.41 rows/s]\n",
      "Processed: 50 rows [00:00, 11279.26 rows/s]\n",
      "Preparing: 50 rows [00:00, 25694.09 rows/s]\n",
      "Processed: 50 rows [01:17,  1.54s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2320.18 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 27795.26 rows/s]\n",
      "Processed: 50 rows [00:00, 11022.56 rows/s]\n",
      "Preparing: 50 rows [00:00, 18301.35 rows/s]\n",
      "Processed: 50 rows [01:09,  1.40s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2147.07 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 30428.79 rows/s]\n",
      "Processed: 50 rows [00:00, 8196.48 rows/s]\n",
      "Preparing: 50 rows [00:00, 25215.25 rows/s]\n",
      "Processed: 50 rows [01:12,  1.46s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2461.45 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25509.69 rows/s]\n",
      "Processed: 50 rows [00:00, 8595.24 rows/s]\n",
      "Preparing: 50 rows [00:00, 16431.50 rows/s]\n",
      "Processed: 50 rows [01:14,  1.48s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3364.19 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 46634.47 rows/s]\n",
      "Processed: 50 rows [00:00, 10288.23 rows/s]\n",
      "Preparing: 50 rows [00:00, 21258.51 rows/s]\n",
      "Processed: 50 rows [01:06,  1.34s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2745.86 tables/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "from datachain import File, DataChain, Column\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "prompt = \"You are assigned to evaluate the success of a dialog between user and a chatbot. Do not reply with a JSON and use Markdown.\"\n",
    "\n",
    "class ActionItem(BaseModel):\n",
    "    team: str \n",
    "    task: str\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    sentiment: str \n",
    "    key_issues: list[str] \n",
    "    action_items: list[ActionItem] \n",
    "\n",
    "def eval_dialogue(client, file: File) -> EvalResponse:\n",
    "\n",
    "     try:\n",
    "         completion = client.beta.chat.completions.parse(\n",
    "             model=\"gpt-4o-2024-08-06\",\n",
    "             messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": file.read()},\n",
    "             ],\n",
    "             response_format=EvalResponse,\n",
    "         )\n",
    "     except Exception as e:\n",
    "         print(f\"OpenAI API error: {e}\")\n",
    "         return EvalResponse(sentiment = \"\", key_issues =[], action_items =[]) \n",
    "     message = completion.choices[0].message\n",
    "     try:\n",
    "         json_dict = ast.literal_eval(str(message.parsed.dict()))\n",
    "     except Exception as e:\n",
    "         # Catch cases where GPT fails to produce valid JSON\n",
    "         print(f\"Parsed message format error: {e}\")\n",
    "         print(message.parsed.dict())\n",
    "         return EvalResponse(sentiment=\"\", key_issues=[], action_items=[])\n",
    "     try:\n",
    "         EvalResponse(**message.parsed.dict())\n",
    "     except ValidationError as e:\n",
    "         print(e)\n",
    "     return message.parsed\n",
    "\n",
    "# 1,000 entries to evaluate\n",
    "for i in range(1,21):\n",
    "    chain = (\n",
    "       DataChain.from_storage(\"gs://datachain-demo/chatbot-KiT/\", object_name=\"file\", type=\"text\")\n",
    "       .setup(client=lambda : OpenAI(api_key=openai_api_key))\n",
    "       .settings(cache=True)\n",
    "       .map(evaluation=eval_dialogue)\n",
    "       .exec()\n",
    "    )\n",
    "    print(f\"{i*50} calls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e556604-3385-4127-aff8-7c17779770d1",
   "metadata": {},
   "source": [
    "For the example above,  approximately one run in 2,000 fails to produce a valid JSON.\n",
    "The failure is typically an OpenAI giving up during parsing:\n",
    "\n",
    "```\n",
    "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/lib/_parsing/_completions.py\", line 72, in parse_chat_completion\n",
    "    raise LengthFinishReasonError()\n",
    "openai.LengthFinishReasonError: Could not parse response content as the length limit was reached\n",
    "```\n",
    "\n",
    "In a rare occasion, the model can also produce a malformed JSON dictionary:\n",
    "\n",
    "```\n",
    "pydantic_core._pydantic_core.ValidationError: 1 validation error for Evaluation\n",
    "  Invalid JSON: EOF while parsing a string at line 1 column 5641 [type=json_invalid, input_value='{\"outcome\":\"No\",\"explana...ure bots! ! \\\\n++++++++', input_type=str]\n",
    "    For further information visit https://errors.pydantic.dev/2.8/v/json_invalid\n",
    "Processed: 49 rows [02:38,  3.24s/ rows]\n",
    "```\n",
    "\n",
    "Either way, the failire rate for OpenAI's Structured Outputs still seem to somewhat dependent on the prompt.\n",
    "\n",
    "* Second, the OpenAI API reads Pydantic field descriptions and *tries* to sticks to them. In the example below, the field \"suggestions\" is set to validates for exactly six entries, the field \"outcome\" validates to two literals, and suggestions must start with \"K\". As we can see from the run, transgressions do happen but they are infrequent. Moreover, simpler instructions appear to be more reliable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685a4249-1e44-47b8-9a70-d5d20c8f815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 21153.44 rows/s]\n",
      "Processed: 50 rows [00:00, 9248.74 rows/s]\n",
      "Preparing: 50 rows [00:00, 19004.55 rows/s]\n",
      "Processed: 50 rows [01:59,  2.38s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4214.32 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 37395.72 rows/s]\n",
      "Processed: 50 rows [00:00, 12109.67 rows/s]\n",
      "Preparing: 50 rows [00:00, 20097.29 rows/s]\n",
      "Processed: 9 rows [00:18,  2.28s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [01:52,  2.25s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2262.60 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 33372.88 rows/s]\n",
      "Processed: 50 rows [00:00, 11641.14 rows/s]\n",
      "Preparing: 50 rows [00:00, 24283.84 rows/s]\n",
      "Processed: 7 rows [00:13,  2.20s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [01:47,  2.15s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2346.14 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 38699.98 rows/s]\n",
      "Processed: 50 rows [00:00, 13202.92 rows/s]\n",
      "Preparing: 50 rows [00:00, 24836.00 rows/s]\n",
      "Processed: 3 rows [00:04,  1.75s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 4 rows [00:07,  2.14s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 31 rows [01:04,  2.01s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [01:42,  2.06s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2281.68 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 33251.18 rows/s]\n",
      "Processed: 50 rows [00:00, 12165.87 rows/s]\n",
      "Preparing: 50 rows [00:00, 21711.90 rows/s]\n",
      "Processed: 48 rows [01:42,  1.93s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array length of 6 not followed: [Suggestion(suggestion='Keep improving the natural language understanding')]\n",
      "Array length of 6 not followed: [Suggestion(suggestion='Keep improving the natural language understanding')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [01:46,  2.13s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2459.64 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26122.97 rows/s]\n",
      "Processed: 50 rows [00:00, 9331.46 rows/s]\n",
      "Preparing: 50 rows [00:00, 15660.91 rows/s]\n",
      "Processed: 50 rows [01:59,  2.38s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3784.62 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 26096.96 rows/s]\n",
      "Processed: 50 rows [00:00, 9551.18 rows/s]\n",
      "Preparing: 50 rows [00:00, 19252.29 rows/s]\n",
      "Processed: 50 rows [02:09,  2.58s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2372.68 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 38793.04 rows/s]\n",
      "Processed: 50 rows [00:00, 10419.08 rows/s]\n",
      "Preparing: 50 rows [00:00, 16562.57 rows/s]\n",
      "Processed: 50 rows [02:41,  3.23s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3959.69 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 33151.31 rows/s]\n",
      "Processed: 50 rows [00:00, 10933.49 rows/s]\n",
      "Preparing: 50 rows [00:00, 19799.40 rows/s]\n",
      "Processed: 50 rows [01:56,  2.32s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2791.55 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 31230.86 rows/s]\n",
      "Processed: 50 rows [00:00, 11935.30 rows/s]\n",
      "Preparing: 50 rows [00:00, 28324.58 rows/s]\n",
      "Processed: 4 rows [00:06,  1.75s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 39 rows [01:33,  2.60s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array length of 6 not followed: [Suggestion(suggestion='Knowledge-base improvement to handle inappropriate language and flag concerns')]\n",
      "Array length of 6 not followed: [Suggestion(suggestion='Knowledge-base improvement to handle inappropriate language and flag concerns')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [02:01,  2.43s/ rows]\n",
      "Cleanup: 4 tables [00:00, 4140.48 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 13901.31 rows/s]\n",
      "Processed: 50 rows [00:00, 7024.46 rows/s]\n",
      "Preparing: 50 rows [00:00, 14867.09 rows/s]\n",
      "Processed: 50 rows [01:48,  2.17s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2375.37 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 39272.51 rows/s]\n",
      "Processed: 50 rows [00:00, 11539.30 rows/s]\n",
      "Preparing: 50 rows [00:00, 20938.02 rows/s]\n",
      "Processed: 38 rows [01:22,  2.24s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [01:47,  2.14s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3018.57 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 21375.52 rows/s]\n",
      "Processed: 50 rows [00:00, 9536.84 rows/s]\n",
      "Preparing: 50 rows [00:00, 21204.77 rows/s]\n",
      "Processed: 49 rows [01:42,  2.26s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [01:44,  2.10s/ rows]\n",
      "Cleanup: 4 tables [00:00, 1755.12 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 19127.62 rows/s]\n",
      "Processed: 50 rows [00:00, 6852.99 rows/s]\n",
      "Preparing: 50 rows [00:00, 17973.53 rows/s]\n",
      "Processed: 50 rows [02:00,  2.40s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2730.22 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25906.76 rows/s]\n",
      "Processed: 50 rows [00:00, 12297.13 rows/s]\n",
      "Preparing: 50 rows [00:00, 19239.93 rows/s]\n",
      "Processed: 50 rows [02:01,  2.43s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2094.27 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 40556.02 rows/s]\n",
      "Processed: 50 rows [00:00, 8819.72 rows/s]\n",
      "Preparing: 50 rows [00:00, 27053.04 rows/s]\n",
      "Processed: 50 rows [02:29,  2.99s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2350.41 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25460.14 rows/s]\n",
      "Processed: 50 rows [00:00, 8252.28 rows/s]\n",
      "Preparing: 50 rows [00:00, 20550.24 rows/s]\n",
      "Processed: 50 rows [02:10,  2.61s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2557.89 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25148.72 rows/s]\n",
      "Processed: 50 rows [00:00, 8676.31 rows/s]\n",
      "Preparing: 50 rows [00:00, 15464.58 rows/s]\n",
      "Processed: 50 rows [01:53,  2.28s/ rows]\n",
      "Cleanup: 4 tables [00:00, 3062.65 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 25472.51 rows/s]\n",
      "Processed: 50 rows [00:00, 13896.71 rows/s]\n",
      "Preparing: 50 rows [00:00, 20883.81 rows/s]\n",
      "Processed: 48 rows [01:48,  1.84s/ rows]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 suggestions don't start with K\n",
      "1 suggestions don't start with K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 50 rows [01:53,  2.27s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2305.51 tables/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing: 50 rows [00:00, 23529.14 rows/s]\n",
      "Processed: 50 rows [00:00, 9865.70 rows/s]\n",
      "Preparing: 50 rows [00:00, 21221.94 rows/s]\n",
      "Processed: 50 rows [02:54,  3.48s/ rows]\n",
      "Cleanup: 4 tables [00:00, 2653.78 tables/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "from datachain import File, DataChain, Column\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "prompt = \"You are assigned to evaluate the success of a dialog between user and a chatbot. Reply with JSON.\"\n",
    "\n",
    "class Suggestion(BaseModel):\n",
    "    suggestion: str = Field(description=\"Suggestion to improve the bot, starting with letter K\")\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    outcome: str = Field(description=\"whether a dialog was successful, either Yes or No\")\n",
    "    explanation: str = Field(description=\"rationale behind the decision on outcome\")\n",
    "    suggestions: list[Suggestion] = Field(description=\"Six ways to improve a bot\")\n",
    "    \n",
    "    @field_validator(\"outcome\")\n",
    "    def check_literal(cls, value):\n",
    "        if not (value in [\"Yes\", \"No\"]):\n",
    "            print(f\"Literal Yes/No not followed: {value}\") \n",
    "        return value\n",
    "    \n",
    "    @field_validator(\"suggestions\")\n",
    "    def count_suggestions(cls, value):\n",
    "        if len(value) != 6:\n",
    "            print(f\"Array length of 6 not followed: {value}\")\n",
    "        count = sum(1 for item in value if item.suggestion.startswith('K'))\n",
    "        if len(value) != count:\n",
    "            print(f\"{len(value)-count} suggestions don't start with K\")\n",
    "        return value\n",
    "\n",
    "def eval_dialogue(client, file: File) -> Evaluation:\n",
    "     completion = client.beta.chat.completions.parse(\n",
    "         model=\"gpt-4o-2024-08-06\",\n",
    "         messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": file.read()},\n",
    "         ],\n",
    "         response_format=Evaluation,\n",
    "     )\n",
    "     message = completion.choices[0].message\n",
    "     try:\n",
    "         json_dict = ast.literal_eval(str(message.parsed.dict()))\n",
    "     except Exception as e:\n",
    "         # Catch cases where GPT fails to produce valid JSON\n",
    "         print(f\"Parsed message format error: {e}\")\n",
    "         print(message.parsed.dict())\n",
    "         return Evaluation(outcome=\"\", explanation=\"\", suggestions=[])\n",
    "     try:\n",
    "         Evaluation(**message.parsed.dict())\n",
    "     except ValidationError as e:\n",
    "         print(e)\n",
    "     return message.parsed\n",
    "\n",
    "# 1,000 entries to evaluate\n",
    "for i in range(1,21):\n",
    "    chain = (\n",
    "       DataChain.from_storage(\"gs://datachain-demo/chatbot-KiT/\", object_name=\"file\", type=\"text\")\n",
    "       .setup(client=lambda : OpenAI(api_key=openai_api_key))\n",
    "       .settings(cache=True)\n",
    "       .map(evaluation=eval_dialogue)\n",
    "       .exec()\n",
    "    )\n",
    "    print(f\"{i*50} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d9ed9-ca21-4fa4-9c66-f3a578d2ef93",
   "metadata": {},
   "source": [
    "\n",
    "Unfortunately, OpenAI's documentation states that the built-in Pydantic field constraints (like \"minItems\") are not supported – which leaves it one the user to verify and enforce the content of the objects produced by the LLM.\n",
    "\n",
    "Nonetheless, the direct support for Zod and Pydantic data models is very useful and puts OpenAI ahead of the competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c633b6f-09da-487c-a326-af1eb8a845a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

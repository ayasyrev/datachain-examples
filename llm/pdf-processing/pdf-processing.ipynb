{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datachain and LLMs - Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing PDF documents with DataChain and Unstructured.io\n",
    "\n",
    "Most organizations keep a large source of information in the form of various internal documents, call transcripts and other unstructured data. These data contain a lot of useful insights about customers, employees or the inner workings of the company. However, they remain largely untapped by data teams due to the difficulty of dealing with large quantities of data in unstructured formats.\n",
    "\n",
    "Today, we will see how you can process a collection of documents in less than 60 lines of code and turn them into vector embeddings which are much easier to work with and useful downstream (e.g. as ML features or for RAG applications). This approach is also scalable and you will benefit from easy versioning of the final datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach and tools\n",
    "\n",
    "We will work with a publicly available MSFT azure container which contains a collection of Neurips conference papers (representing our internal company documents).\n",
    "\n",
    "For data processing we will use the  [unstructured](https://github.com/Unstructured-IO/unstructured) Python library which contains a lot of useful functionality for unstructured data processing.\n",
    "\n",
    "With unstructured we will:\n",
    "* Easily process each document\n",
    "* Partition and clean it\n",
    "* Create vector embeddings from the partitions\n",
    "\n",
    "We will also use [DataChain](https://github.com/iterative/datachain), which is an open-source Python data-frame library which helps ML and AI engineers to build a metadata layer on top of unstructured files. That way, we do not have to copy the original files anywhere or load them all to memory, significantly scaling up the volume of data we can process.\n",
    "\n",
    "With [DataChain](https://github.com/iterative/datachain), we will:\n",
    "* Easily search and filter our data container to only load the documents we need\n",
    "* Scale up the document processing with unstructured.io to the level of our entire document collection\n",
    "* Save the results as versioned datasets in tabular format, ready for processing downstream\n",
    "\n",
    "Both libraries can be easily installed with pip.\n",
    "\n",
    "```\n",
    "pip install unstructured datachain\n",
    "```\n",
    "(In this example we are using `unstructured` version `0.15.7`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full working code\n",
    "\n",
    "Here you can have a look at the full code used in our example which you can run (and we will upack in a second) This code will load our document collection with DataChain and create a DataChain UDF (user-defined function) `process_pdf` which will load, partition and clean the text and create vector embeddings using `unstructured`. It then saves (and automatically versions) the resulting dataset containing the embeddings, cleaned human-readable text and a reference key for all of the original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Listing gs://datachain-demo: 738 objects [00:00, 878.91 objects/s]\n",
      "Preparing: 738 rows [00:00, 171328.74 rows/s]\n",
      "Processed: 738 rows [00:00, 11432.34 rows/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/tibor/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/__main__.py\", line 6, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/tibor/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/cli.py\", line 931, in main\n",
      "    from datachain.query.dispatch import udf_entrypoint\n",
      "  File \"/home/tibor/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/query/dispatch.py\", line 22, in <module>\n",
      "    from datachain.query.queue import (\n",
      "  File \"/home/tibor/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/query/queue.py\", line 8, in <module>\n",
      "    import msgpack\n",
      "ModuleNotFoundError: No module named 'msgpack'\n",
      "Cleanup: 3 tables [00:00, 4387.35 tables/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "UDF Execution Failed!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m Chunk(\n\u001b[1;32m     44\u001b[0m             key\u001b[38;5;241m=\u001b[39mfile\u001b[38;5;241m.\u001b[39mpath,\n\u001b[1;32m     45\u001b[0m             text\u001b[38;5;241m=\u001b[39mchunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m     46\u001b[0m             embeddings\u001b[38;5;241m=\u001b[39mchunk\u001b[38;5;241m.\u001b[39membeddings,\n\u001b[1;32m     47\u001b[0m         )\n\u001b[1;32m     49\u001b[0m dc \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     50\u001b[0m     DataChain\u001b[38;5;241m.\u001b[39mfrom_storage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://datachain-demo/neurips\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;241m.\u001b[39msettings(parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(C\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m.\u001b[39mgen(document\u001b[38;5;241m=\u001b[39mprocess_pdf)\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m \u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedded-documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m DataChain\u001b[38;5;241m.\u001b[39mfrom_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedded-documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/lib/dc.py:573\u001b[0m, in \u001b[0;36mDataChain.save\u001b[0;34m(self, name, version)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save to a Dataset. It returns the chain itself.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    version : version of a dataset. Default - the last version that exist.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignals_schema\u001b[38;5;241m.\u001b[39mclone_without_sys_signals()\u001b[38;5;241m.\u001b[39mserialize()\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:1643\u001b[0m, in \u001b[0;36mDatasetQuery.save\u001b[0;34m(self, name, version, feature_schema, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mgenerate_temp_dataset_name()\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1643\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1645\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1646\u001b[0m         c \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01melse\u001b[39;00m Column(c\u001b[38;5;241m.\u001b[39mname, c\u001b[38;5;241m.\u001b[39mtype)\n\u001b[1;32m   1647\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m query\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   1648\u001b[0m     ]\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msys__id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:1186\u001b[0m, in \u001b[0;36mDatasetQuery.apply_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         group_by \u001b[38;5;241m=\u001b[39m step\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_table_names\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# a chain of steps linked by results\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies\u001b[38;5;241m.\u001b[39mupdate(result\u001b[38;5;241m.\u001b[39mdependencies)\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group_by:\n",
      "File \u001b[0;32m~/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:637\u001b[0m, in \u001b[0;36mUDFStep.apply\u001b[0;34m(self, query_generator, temp_tables)\u001b[0m\n\u001b[1;32m    635\u001b[0m udf_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_udf_table(_query)\n\u001b[1;32m    636\u001b[0m temp_tables\u001b[38;5;241m.\u001b[39mappend(udf_table\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 637\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate_udf_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mudf_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m q, cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_result_query(udf_table, query)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_result(q, cols)\n",
      "File \u001b[0;32m~/Repos/datachain-examples/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:529\u001b[0m, in \u001b[0;36mUDFStep.populate_udf_table\u001b[0;34m(self, udf_table, query)\u001b[0m\n\u001b[1;32m    522\u001b[0m     result \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(  \u001b[38;5;66;03m# noqa: S603\u001b[39;00m\n\u001b[1;32m    523\u001b[0m         [\u001b[38;5;241m*\u001b[39mexec_cmd, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal-run-udf\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mprocess_data,\n\u001b[1;32m    525\u001b[0m         check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    526\u001b[0m         env\u001b[38;5;241m=\u001b[39menvs,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUDF Execution Failed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# Otherwise process single-threaded (faster for smaller UDFs)\u001b[39;00m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;66;03m# Optionally instantiate the UDF instance if a class is provided.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mudf, UDFFactory):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: UDF Execution Failed!"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from datachain import DataChain, C, File, DataModel\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "from unstructured.cleaners.core import clean\n",
    "from unstructured.cleaners.core import replace_unicode_quotes\n",
    "from unstructured.cleaners.core import group_broken_paragraphs\n",
    "\n",
    "from unstructured.embed.huggingface import HuggingFaceEmbeddingConfig, HuggingFaceEmbeddingEncoder\n",
    "\n",
    "# Define the output as a DataModel class\n",
    "class Chunk(DataModel):\n",
    "    key: str\n",
    "    text: str\n",
    "    embeddings: List[float]\n",
    "\n",
    "# Define embedding encoder\n",
    "\n",
    "embedding_encoder = HuggingFaceEmbeddingEncoder(\n",
    "     config=HuggingFaceEmbeddingConfig()\n",
    ")\n",
    "\n",
    "# Use signatures to define UDF input/output (these can be pydantic model or regular Python types)\n",
    "def process_pdf(file: File) -> Iterator[Chunk]:\n",
    "    # Ingest the file\n",
    "    with file.open() as f:\n",
    "        chunks = partition_pdf(file=f, chunking_strategy=\"by_title\", strategy=\"fast\")\n",
    "\n",
    "    # Clean the chunks and add new columns\n",
    "    for chunk in chunks:\n",
    "        chunk.apply(lambda text: clean(text, bullets=True, extra_whitespace=True, trailing_punctuation=True))\n",
    "        chunk.apply(replace_unicode_quotes)\n",
    "        chunk.apply(group_broken_paragraphs)\n",
    "\n",
    "    # create embeddings\n",
    "    chunks_embedded = embedding_encoder.embed_documents(chunks)\n",
    "\n",
    "    # Add new rows to DataChain\n",
    "    for chunk in chunks_embedded:\n",
    "        yield Chunk(\n",
    "            key=file.path,\n",
    "            text=chunk.text,\n",
    "            embeddings=chunk.embeddings,\n",
    "        )\n",
    "\n",
    "dc = (\n",
    "    DataChain.from_storage(\"gs://datachain-demo/neurips\")\n",
    "    .filter(C.file.path.glob(\"*.pdf\"))\n",
    "    .gen(document=process_pdf)\n",
    ")\n",
    "\n",
    "dc.save(\"embedded-documents\")\n",
    "\n",
    "DataChain.from_dataset(\"embedded-documents\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and saving the DataChain\n",
    "\n",
    "The following few lines of code are all that we need to load and select the right data in from our storage and to process them with `unstructured`.\n",
    "\n",
    "```python\n",
    "dc = (\n",
    "    DataChain.from_storage(\"gs://datachain-demo/neurips\")\n",
    "    .settings(parallel=-1)\n",
    "    .filter(C.file.path.glob(\"*.pdf\"))\n",
    "    .gen(document=process_pdf)\n",
    ")\n",
    "```\n",
    "\n",
    "Let's unpack:\n",
    "\n",
    "The `from_storage` and `filter` methods allow us to ingest the data from a bucket/storage container. Since DataChain uses lazy evaluation, no other files than those specified by the `filter` will be loaded, speeding up the process considerably. Setting `parallel` to `-1` tells DataChain to make use of all CPUs/cores on the current machine, speeding things up once again.\n",
    "\n",
    "This will create a DataChain metadata table containing all the information needed to process our PDF files without actually having to copy the files themselves or load them all to memory. Since DataChain operates on a metadata level, it can scale up billions of files without us having to worry about memory overflows.\n",
    "\n",
    "The `gen` method allows us to modify the datachain table and create new rows (potentially more than one per original table row) using datachain UDF functions. Here, the UDF `process_pdf` does all the individual PDF processing with `unstructured`.\n",
    "\n",
    "Finally, we save the table as a dataset by calling \n",
    "\n",
    "```python\n",
    "dc.save(\"embedded-documents\")\n",
    "```\n",
    "\n",
    "This will persist the table and version it (each time we call this command a new version is created automatically). We can then load and display it by the following command, optionally specifying the dataset version\n",
    "\n",
    "```python\n",
    "DataChain.from_dataset(\"embeddings\", version=1).show()\n",
    "```\n",
    "\n",
    "All that's missing is the DataChain UDF definition, so let's see how we do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the UDF\n",
    "\n",
    "The `process_pdf` UDF will take the original datachain table and produce an output with `embeddings` of processed document chunks. We also want to keep the original `text` of each processed document chunk and a `key` by which we can link each chunk back to the original full document.\n",
    "\n",
    "We first specify what we actually want to receive on the output of our UDF by defining the `DataModel`-based `Chunk` class and defining the output column types:\n",
    "\n",
    "```python\n",
    "# Define the output as a DataModel class\n",
    "class Chunk(DataModel):\n",
    "    key: str\n",
    "    text: str\n",
    "    embeddings: List[float]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the `process_pdf` function itself. We use Python signatures to specify the input and output. Here, `File` is a class used by Datachain to refer to the original file - a PDF document in our case. On the output we use `Iterator` since our function will produce multiple chunks (and so multiple rows in our DataChain table) per original file.\n",
    "\n",
    "```python\n",
    "def process_pdf(file: File) -> Iterator[Chunk]:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the function definition as well as the definition of `embedding_encoder` specifies how `unstructured` is used to process each individual PDF file. For more detail on how this is done you can check out the tutorial in the [unstructured documentation](https://docs.unstructured.io/open-source/core-functionality/overview).\n",
    "\n",
    "Finally, we want the UDF to produce new rows in our DataChain table and so we have it return the Chunk objects we specified above. Here, we use `yield` instead of `return` as each PDF file produces several Chunk objects.\n",
    "\n",
    "```python\n",
    "    # Add new rows to DataChain\n",
    "    for chunk in chunks_embedded:\n",
    "        yield Chunk(\n",
    "            key=file.path,\n",
    "            text=chunk.text,\n",
    "            embeddings=chunk.embeddings,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have processed our collection of documents to create a dataset of embeddings. We could now proceed \n",
    "\n",
    "Stay tuned for Part II, where we will explore a use-case where we will use a similar approach to process call centre recordings to create and process transcripts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
